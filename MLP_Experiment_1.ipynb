{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP Experiment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xieruishen/MachineLearningNotebooks/blob/master/MLP_Experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhgxdbGuu0td",
        "colab_type": "code",
        "outputId": "b4b1f40e-1b93-4507-d1d5-26ba973174a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "!pip install torchviz\n",
        "!pip install pycoco"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.2.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=e952246ed09433037e7137fc24f94b1390f493fe5a5896bbf1c3934c3fe958aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "Collecting pycoco\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/b1/8a4e0663e2c9184fc9c800d4691d98cd86ffa21f2b2aeec39bec4386a5a8/pycoco-0.7.2.tar.bz2\n",
            "Collecting ll-xist>=3.9 (from pycoco)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/1d/7c425cfb73d4a5c8c8bbac69a07d301a77bc52913554357a4cb32b917a07/ll-xist-5.53.tar.gz (704kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 5.1MB/s \n",
            "\u001b[?25hCollecting cssutils==1.0.2 (from ll-xist>=3.9->pycoco)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/a9fb9010f58d1c55dd0b7779db2334feb9a572d407024f39a60f44293861/cssutils-1.0.2-py3-none-any.whl (406kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 36.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycoco, ll-xist\n",
            "  Building wheel for pycoco (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycoco: filename=pycoco-0.7.2-cp36-none-any.whl size=9791 sha256=25a9daff90d8ba2dd66c757c406b1818d9d48be33737864de1166da5d5b5683b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/9f/9c/6f40b261f0abad9f29cfe967547d036e4bcfa6a225e442a678\n",
            "  Building wheel for ll-xist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ll-xist: filename=ll_xist-5.53-cp36-cp36m-linux_x86_64.whl size=606482 sha256=74cd3772ba555ac936553ea0d76c6358043c3d0c480d7d47166791b9c2bb8d80\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/23/56/de77a624279e15511955acd3a02fa8deece242ffbffc66d781\n",
            "Successfully built pycoco ll-xist\n",
            "Installing collected packages: cssutils, ll-xist, pycoco\n",
            "Successfully installed cssutils-1.0.2 ll-xist-5.53 pycoco-0.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKYTJtOdph-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import gdown\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysOutsiWxj2I",
        "colab_type": "code",
        "outputId": "40128790-b1e4-403e-9b07-0ca5ed9b62c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import gdown\n",
        "#https://drive.google.com/open?id=1Fn29oJ4kT3dp9Hd3NnAicblRRPKg_5XN\n",
        "#'https://drive.google.com/uc?authuser=0&id=1M3doqupItS419I6z-D3rCHUaPo93HbUE&export=download'\n",
        "data_zip=gdown.download('https://drive.google.com/uc?authuser=0&id=1Fn29oJ4kT3dp9Hd3NnAicblRRPKg_5XN&export=download',  'asl-alphabet.zip', quiet=False)\n",
        "# Create a ZipFile Object and load sample.zip in it"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1Fn29oJ4kT3dp9Hd3NnAicblRRPKg_5XN&export=download\n",
            "To: /content/asl-alphabet.zip\n",
            "3.01GB [00:13, 229MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvQp0GY_KSuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq -o asl-alphabet.zip\n",
        "!cd asl-alphabet/asl_alphabet_train/asl_alphabet_train && rm -r del"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vO4uSsOXSru",
        "colab_type": "code",
        "outputId": "6c556dbd-bfa1-4623-eded-8685b496f5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!cd asl-alphabet/asl_alphabet_train/asl_alphabet_train && ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A  C  E  G  I  K  M  nothing  P  R  space  U  W  Y\n",
            "B  D  F  H  J  L  N  O\t      Q  S  T\t   V  X  Z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCxbu9vJekgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_len = 28\n",
        "class_mappings = {'A': 0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25,'space':26,'nothing':27}\n",
        "class_upgrade={0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y',25:'Z',26:'space',27:'nothing'}\n",
        "# Define a function to parse a sample label into a form that we want (aka a tensor)\n",
        "def get_classes(target):\n",
        "  class_tensor = torch.zeros((class_len),dtype=torch.long)\n",
        "  class_tensor[target] = 1\n",
        "  return class_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CwrB49pz0Wc",
        "colab_type": "code",
        "outputId": "343b5d70-7cfc-47b2-fa4f-f0932ef96762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(img_tensor.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "cal_tech = ImageFolder(root='asl-alphabet/asl_alphabet_train/asl_alphabet_train/', transform=transforms.Compose([transforms.CenterCrop((200,200)),\n",
        "                                                    transforms.Resize((120,120)),\n",
        "                                                    transforms.ToTensor()]))\n",
        "\n",
        "im, target = cal_tech[70110]\n",
        "show_image(im)\n",
        "print(im.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGniPAwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 120])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UfvFPtQWI90",
        "colab_type": "code",
        "outputId": "e5b4ddd8-df95-4cf0-81c2-2a94eeb1958a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "cal_tech_t = ImageFolder(root='asl-alphabet/asl_alphabet_test/asl_alphabet_test/', transform=transforms.Compose([transforms.CenterCrop((200,200)),\n",
        "                                                    transforms.Resize((120,120)),\n",
        "                                                    transforms.ToTensor()]))\n",
        "\n",
        "im_t, target_t = cal_tech_t[1]\n",
        "show_image(im_t)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZMunOYmMGnShdP/C7nGI9K0eFboAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWmxW1VYTiPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set=cal_tech\n",
        "test_set=cal_tech_t\n",
        "image_dims= 3,120,120\n",
        "n_training_samples=84000\n",
        "n_test_samples=20\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPYwk3xOSbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(myCNN, self).__init__()\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
        "        self.fc1_size = 256\n",
        "        self.fc2_size = class_len\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], 32, kernel_size=3,\n",
        "                  stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        # tensor height width and depth\n",
        "        self.maxpool_output_size = int(256 * (image_dims[1]/40) * (image_dims[2] / 40))\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, self.fc1_size)\n",
        "        self.fc2 = nn.Linear(self.fc1_size, self.fc2_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convolutional Layers\n",
        "        x = self.activation_func(self.pool2(self.conv1(x)))\n",
        "        x = self.activation_func(self.pool2(self.conv2(x)))\n",
        "        x = self.activation_func(self.pool2(self.conv3(x)))\n",
        "        x = self.activation_func(self.pool5(self.conv4(x)))\n",
        "        # Fully Connected Layers\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function, we'll use BCE or Binary CrossEntropy that does not assume one class fer example\n",
        "      # https://pytorch.org/docs/stable/nn.html\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      #loss = nn.BCEWithLogitsLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUq6FTN2BDR_",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "First let's create our model. Let's also check out a graphical representation of our model (using a library we downloaded earlier) to validate the model looks like we think it should.  This is definitely not the prettiest visualization, and there are lots of things included in here that are related to doing the backward pass (to compute the gradients).  Of particular relevance are the blue nodes, which tell you about the various model parameters and layers.\n",
        "\n",
        "**Running the below cell will override your model if have already trained one**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZVwXv7PAq7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b558b51-b7b9-41b2-f78b-3bd2a8fc170a"
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1,image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net_example = myCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net_example.to(device)\n",
        "\n",
        "visualize_network(net_example)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f014664cc88>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"468pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 468.38 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.8268 .8268) rotate(0) translate(4 1041)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1041 562.5,-1041 562.5,4 -4,4\"/>\n<!-- 139643452705984 -->\n<g id=\"node1\" class=\"node\">\n<title>139643452705984</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"471,-21 367,-21 367,0 471,0 471,-21\"/>\n<text text-anchor=\"middle\" x=\"419\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139643452706040 -->\n<g id=\"node2\" class=\"node\">\n<title>139643452706040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"354,-91 300,-91 300,-57 354,-57 354,-91\"/>\n<text text-anchor=\"middle\" x=\"327\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"327\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (28)</text>\n</g>\n<!-- 139643452706040&#45;&gt;139643452705984 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139643452706040&#45;&gt;139643452705984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M351.6543,-56.9832C365.1894,-47.641 381.8926,-36.1122 395.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"397.2865,-29.7398 403.5283,-21.1788 393.3102,-23.9788 397.2865,-29.7398\"/>\n</g>\n<!-- 139643452706096 -->\n<g id=\"node3\" class=\"node\">\n<title>139643452706096</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"466,-84.5 372,-84.5 372,-63.5 466,-63.5 466,-84.5\"/>\n<text text-anchor=\"middle\" x=\"419\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139643452706096&#45;&gt;139643452705984 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139643452706096&#45;&gt;139643452705984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M419,-63.2281C419,-54.5091 419,-41.9699 419,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"422.5001,-31.1128 419,-21.1128 415.5001,-31.1129 422.5001,-31.1128\"/>\n</g>\n<!-- 139643452706264 -->\n<g id=\"node4\" class=\"node\">\n<title>139643452706264</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"470,-154.5 366,-154.5 366,-133.5 470,-133.5 470,-154.5\"/>\n<text text-anchor=\"middle\" x=\"418\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139643452706264&#45;&gt;139643452706096 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139643452706264&#45;&gt;139643452706096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M418.1519,-133.3685C418.2972,-123.1925 418.5206,-107.5606 418.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"422.2034,-94.7806 418.8467,-84.7315 415.2041,-94.6805 422.2034,-94.7806\"/>\n</g>\n<!-- 139643452706376 -->\n<g id=\"node5\" class=\"node\">\n<title>139643452706376</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"354,-231 300,-231 300,-197 354,-197 354,-231\"/>\n<text text-anchor=\"middle\" x=\"327\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"327\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139643452706376&#45;&gt;139643452706264 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139643452706376&#45;&gt;139643452706264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M349.4944,-196.6966C363.7034,-185.7666 381.9745,-171.7119 396.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"398.447,-163.4565 404.2393,-154.5852 394.179,-157.9081 398.447,-163.4565\"/>\n</g>\n<!-- 139643452706432 -->\n<g id=\"node6\" class=\"node\">\n<title>139643452706432</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"463.5,-224.5 372.5,-224.5 372.5,-203.5 463.5,-203.5 463.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"418\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139643452706432&#45;&gt;139643452706264 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139643452706432&#45;&gt;139643452706264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M418,-203.3685C418,-193.1925 418,-177.5606 418,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.5001,-164.7315 418,-154.7315 414.5001,-164.7316 421.5001,-164.7315\"/>\n</g>\n<!-- 139643452706600 -->\n<g id=\"node7\" class=\"node\">\n<title>139643452706600</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"464,-294.5 370,-294.5 370,-273.5 464,-273.5 464,-294.5\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139643452706600&#45;&gt;139643452706432 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139643452706600&#45;&gt;139643452706432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417.1519,-273.3685C417.2972,-263.1925 417.5206,-247.5606 417.7016,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.2034,-234.7806 417.8467,-224.7315 414.2041,-234.6805 421.2034,-234.7806\"/>\n</g>\n<!-- 139643452706712 -->\n<g id=\"node8\" class=\"node\">\n<title>139643452706712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"507,-358 327,-358 327,-337 507,-337 507,-358\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139643452706712&#45;&gt;139643452706600 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139643452706712&#45;&gt;139643452706600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-336.7281C417,-328.0091 417,-315.4699 417,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-304.6128 417,-294.6128 413.5001,-304.6129 420.5001,-304.6128\"/>\n</g>\n<!-- 139643452760192 -->\n<g id=\"node9\" class=\"node\">\n<title>139643452760192</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"495.5,-415 338.5,-415 338.5,-394 495.5,-394 495.5,-415\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139643452760192&#45;&gt;139643452706712 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139643452760192&#45;&gt;139643452706712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-393.7787C417,-386.6134 417,-376.9517 417,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-368.1732 417,-358.1732 413.5001,-368.1732 420.5001,-368.1732\"/>\n</g>\n<!-- 139643452760360 -->\n<g id=\"node10\" class=\"node\">\n<title>139643452760360</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"352,-478.5 258,-478.5 258,-457.5 352,-457.5 352,-478.5\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-464.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139643452760360&#45;&gt;139643452760192 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139643452760360&#45;&gt;139643452760192</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.7463,-457.3715C341.675,-447.2066 368.9273,-431.7555 389.3486,-420.1774\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.2197,-423.14 398.1926,-415.1631 387.7672,-417.0506 391.2197,-423.14\"/>\n</g>\n<!-- 139643452760584 -->\n<g id=\"node11\" class=\"node\">\n<title>139643452760584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"395,-542 215,-542 215,-521 395,-521 395,-542\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-528.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139643452760584&#45;&gt;139643452760360 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139643452760584&#45;&gt;139643452760360</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-520.7281C305,-512.0091 305,-499.4699 305,-488.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-488.6128 305,-478.6128 301.5001,-488.6129 308.5001,-488.6128\"/>\n</g>\n<!-- 139643452760752 -->\n<g id=\"node12\" class=\"node\">\n<title>139643452760752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"383.5,-599 226.5,-599 226.5,-578 383.5,-578 383.5,-599\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-585.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139643452760752&#45;&gt;139643452760584 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139643452760752&#45;&gt;139643452760584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-577.7787C305,-570.6134 305,-560.9517 305,-552.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-552.1732 305,-542.1732 301.5001,-552.1732 308.5001,-552.1732\"/>\n</g>\n<!-- 139643452760920 -->\n<g id=\"node13\" class=\"node\">\n<title>139643452760920</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"243,-662.5 149,-662.5 149,-641.5 243,-641.5 243,-662.5\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-648.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139643452760920&#45;&gt;139643452760752 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139643452760920&#45;&gt;139643452760752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.4904,-641.2281C231.8986,-631.0866 258.1791,-615.7764 277.9437,-604.2622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"279.9039,-607.1709 286.7827,-599.1128 276.3802,-601.1224 279.9039,-607.1709\"/>\n</g>\n<!-- 139643452761144 -->\n<g id=\"node14\" class=\"node\">\n<title>139643452761144</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"286,-726 106,-726 106,-705 286,-705 286,-726\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-712.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139643452761144&#45;&gt;139643452760920 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139643452761144&#45;&gt;139643452760920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-704.7281C196,-696.0091 196,-683.4699 196,-672.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-672.6128 196,-662.6128 192.5001,-672.6129 199.5001,-672.6128\"/>\n</g>\n<!-- 139643452761312 -->\n<g id=\"node15\" class=\"node\">\n<title>139643452761312</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"274.5,-783 117.5,-783 117.5,-762 274.5,-762 274.5,-783\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-769.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139643452761312&#45;&gt;139643452761144 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139643452761312&#45;&gt;139643452761144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-761.7787C196,-754.6134 196,-744.9517 196,-736.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-736.1732 196,-726.1732 192.5001,-736.1732 199.5001,-736.1732\"/>\n</g>\n<!-- 139643452761480 -->\n<g id=\"node16\" class=\"node\">\n<title>139643452761480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"137,-846.5 43,-846.5 43,-825.5 137,-825.5 137,-846.5\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-832.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139643452761480&#45;&gt;139643452761312 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139643452761480&#45;&gt;139643452761312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M107.9815,-825.2281C124.9106,-815.0866 150.4677,-799.7764 169.6884,-788.2622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"171.5043,-791.2544 178.2841,-783.1128 167.9069,-785.2494 171.5043,-791.2544\"/>\n</g>\n<!-- 139643452761704 -->\n<g id=\"node17\" class=\"node\">\n<title>139643452761704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"180,-910 0,-910 0,-889 180,-889 180,-910\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-896.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139643452761704&#45;&gt;139643452761480 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139643452761704&#45;&gt;139643452761480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90,-888.7281C90,-880.0091 90,-867.4699 90,-856.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.5001,-856.6128 90,-846.6128 86.5001,-856.6129 93.5001,-856.6128\"/>\n</g>\n<!-- 139643452761872 -->\n<g id=\"node18\" class=\"node\">\n<title>139643452761872</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"168.5,-967 11.5,-967 11.5,-946 168.5,-946 168.5,-967\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-953.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139643452761872&#45;&gt;139643452761704 -->\n<g id=\"edge17\" class=\"edge\">\n<title>139643452761872&#45;&gt;139643452761704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90,-945.7787C90,-938.6134 90,-928.9517 90,-920.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.5001,-920.1732 90,-910.1732 86.5001,-920.1732 93.5001,-920.1732\"/>\n</g>\n<!-- 139643452762040 -->\n<g id=\"node19\" class=\"node\">\n<title>139643452762040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"84.5,-1037 3.5,-1037 3.5,-1003 84.5,-1003 84.5,-1037\"/>\n<text text-anchor=\"middle\" x=\"44\" y=\"-1023.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"44\" y=\"-1010.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 3, 3, 3)</text>\n</g>\n<!-- 139643452762040&#45;&gt;139643452761872 -->\n<g id=\"edge18\" class=\"edge\">\n<title>139643452762040&#45;&gt;139643452761872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M56.3272,-1002.9832C62.4107,-994.5853 69.7742,-984.4204 76.0621,-975.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"79.0945,-977.5204 82.1266,-967.3687 73.4256,-973.4138 79.0945,-977.5204\"/>\n</g>\n<!-- 139643452762096 -->\n<g id=\"node20\" class=\"node\">\n<title>139643452762096</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"171,-1037 103,-1037 103,-1003 171,-1003 171,-1037\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-1023.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"137\" y=\"-1010.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 139643452762096&#45;&gt;139643452761872 -->\n<g id=\"edge19\" class=\"edge\">\n<title>139643452762096&#45;&gt;139643452761872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M124.4049,-1002.9832C118.1237,-994.4969 110.5069,-984.2062 104.0384,-975.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.8071,-973.3243 98.0445,-967.3687 101.1806,-977.4888 106.8071,-973.3243\"/>\n</g>\n<!-- 139643452761536 -->\n<g id=\"node21\" class=\"node\">\n<title>139643452761536</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"236.5,-853 155.5,-853 155.5,-819 236.5,-819 236.5,-853\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-839.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"196\" y=\"-826.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 32, 3, 3)</text>\n</g>\n<!-- 139643452761536&#45;&gt;139643452761312 -->\n<g id=\"edge20\" class=\"edge\">\n<title>139643452761536&#45;&gt;139643452761312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-818.9832C196,-811.1157 196,-801.6973 196,-793.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-793.3686 196,-783.3687 192.5001,-793.3687 199.5001,-793.3686\"/>\n</g>\n<!-- 139643452761592 -->\n<g id=\"node22\" class=\"node\">\n<title>139643452761592</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"323,-853 255,-853 255,-819 323,-819 323,-853\"/>\n<text text-anchor=\"middle\" x=\"289\" y=\"-839.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"289\" y=\"-826.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139643452761592&#45;&gt;139643452761312 -->\n<g id=\"edge21\" class=\"edge\">\n<title>139643452761592&#45;&gt;139643452761312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M264.0777,-818.9832C250.3955,-809.641 233.5107,-798.1122 219.9799,-788.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"221.872,-785.9273 211.6398,-783.1788 217.9248,-791.7082 221.872,-785.9273\"/>\n</g>\n<!-- 139643452760976 -->\n<g id=\"node23\" class=\"node\">\n<title>139643452760976</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"348.5,-669 261.5,-669 261.5,-635 348.5,-635 348.5,-669\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv3.weight</text>\n<text text-anchor=\"middle\" x=\"305\" y=\"-642.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 64, 3, 3)</text>\n</g>\n<!-- 139643452760976&#45;&gt;139643452760752 -->\n<g id=\"edge22\" class=\"edge\">\n<title>139643452760976&#45;&gt;139643452760752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-634.9832C305,-627.1157 305,-617.6973 305,-609.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-609.3686 305,-599.3687 301.5001,-609.3687 308.5001,-609.3686\"/>\n</g>\n<!-- 139643452761032 -->\n<g id=\"node24\" class=\"node\">\n<title>139643452761032</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"435,-669 367,-669 367,-635 435,-635 435,-669\"/>\n<text text-anchor=\"middle\" x=\"401\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv3.bias</text>\n<text text-anchor=\"middle\" x=\"401\" y=\"-642.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 139643452761032&#45;&gt;139643452760752 -->\n<g id=\"edge23\" class=\"edge\">\n<title>139643452761032&#45;&gt;139643452760752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M375.2738,-634.9832C361.1502,-625.641 343.7208,-614.1122 329.7534,-604.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"331.4158,-601.7766 321.1444,-599.1788 327.5539,-607.6149 331.4158,-601.7766\"/>\n</g>\n<!-- 139643452760416 -->\n<g id=\"node25\" class=\"node\">\n<title>139643452760416</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"463.5,-485 370.5,-485 370.5,-451 463.5,-451 463.5,-485\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv4.weight</text>\n<text text-anchor=\"middle\" x=\"417\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 128, 3, 3)</text>\n</g>\n<!-- 139643452760416&#45;&gt;139643452760192 -->\n<g id=\"edge24\" class=\"edge\">\n<title>139643452760416&#45;&gt;139643452760192</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-450.9832C417,-443.1157 417,-433.6973 417,-425.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-425.3686 417,-415.3687 413.5001,-425.3687 420.5001,-425.3686\"/>\n</g>\n<!-- 139643452760472 -->\n<g id=\"node26\" class=\"node\">\n<title>139643452760472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"550,-485 482,-485 482,-451 550,-451 550,-485\"/>\n<text text-anchor=\"middle\" x=\"516\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv4.bias</text>\n<text text-anchor=\"middle\" x=\"516\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139643452760472&#45;&gt;139643452760192 -->\n<g id=\"edge25\" class=\"edge\">\n<title>139643452760472&#45;&gt;139643452760192</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M489.4698,-450.9832C474.7662,-441.552 456.5881,-429.8924 442.1164,-420.61\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.9558,-417.6318 433.6489,-415.1788 440.1765,-423.5239 443.9558,-417.6318\"/>\n</g>\n<!-- 139643452706488 -->\n<g id=\"node27\" class=\"node\">\n<title>139643452706488</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"555.5,-224.5 482.5,-224.5 482.5,-203.5 555.5,-203.5 555.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"519\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139643452706488&#45;&gt;139643452706264 -->\n<g id=\"edge26\" class=\"edge\">\n<title>139643452706488&#45;&gt;139643452706264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M503.6603,-203.3685C487.1024,-191.8927 460.533,-173.4783 441.3726,-160.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.3659,-157.3219 433.1532,-154.5022 439.3784,-163.0752 443.3659,-157.3219\"/>\n</g>\n<!-- 139643452706656 -->\n<g id=\"node28\" class=\"node\">\n<title>139643452706656</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"557.5,-301 482.5,-301 482.5,-267 557.5,-267 557.5,-301\"/>\n<text text-anchor=\"middle\" x=\"520\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"520\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 2304)</text>\n</g>\n<!-- 139643452706656&#45;&gt;139643452706488 -->\n<g id=\"edge27\" class=\"edge\">\n<title>139643452706656&#45;&gt;139643452706488</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M519.7528,-266.6966C519.6152,-257.0634 519.4429,-245.003 519.2979,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"522.7967,-234.7402 519.1542,-224.7913 515.7975,-234.8403 522.7967,-234.7402\"/>\n</g>\n<!-- 139643452706152 -->\n<g id=\"node29\" class=\"node\">\n<title>139643452706152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"558.5,-84.5 485.5,-84.5 485.5,-63.5 558.5,-63.5 558.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"522\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139643452706152&#45;&gt;139643452705984 -->\n<g id=\"edge28\" class=\"edge\">\n<title>139643452706152&#45;&gt;139643452705984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M504.5275,-63.2281C488.1519,-53.1325 463.4682,-37.9149 444.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.5636,-23.3814 436.2145,-21.1128 442.8901,-29.3401 446.5636,-23.3814\"/>\n</g>\n<!-- 139643452706320 -->\n<g id=\"node30\" class=\"node\">\n<title>139643452706320</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"555.5,-161 488.5,-161 488.5,-127 555.5,-127 555.5,-161\"/>\n<text text-anchor=\"middle\" x=\"522\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"522\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (28, 256)</text>\n</g>\n<!-- 139643452706320&#45;&gt;139643452706152 -->\n<g id=\"edge29\" class=\"edge\">\n<title>139643452706320&#45;&gt;139643452706152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M522,-126.6966C522,-117.0634 522,-105.003 522,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"525.5001,-94.7912 522,-84.7913 518.5001,-94.7913 525.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfj6cHFZvR1I",
        "colab_type": "text"
      },
      "source": [
        "If not use pretrain model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugBh4AxhnbeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training and testing parameters\n",
        "device='cuda'\n",
        "net=myCNN()\n",
        "net.to(device)\n",
        "batch_size = 32\n",
        "test_batch_size = 10\n",
        "learning_rate = 3e-3\n",
        "n_epochs = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSFEkxq1Sbgc",
        "colab_type": "code",
        "outputId": "098beb00-2ecb-4c22-e539-018846bf32f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                        sampler=train_sampler, num_workers=2)\n",
        "print(train_set.classes)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=test_batch_size, sampler=test_sampler, num_workers=2)\n",
        "loss, optimizer = net.get_loss(learning_rate)\n",
        "print(\"loss:\", loss)\n",
        "print(test_set.classes)\n",
        "print(test_loader.dataset)\n",
        "# Define some parameters to keep track of metrics\n",
        "print_every = 20\n",
        "test_every = 200\n",
        "idx = 0\n",
        "train_hist_x = []\n",
        "train_loss_hist = []\n",
        "test_hist_x = []\n",
        "test_loss_hist = []\n",
        "\n",
        "def convert_targets_to_hot_encoding_for_test(targets):\n",
        "  targets_tensor = torch.zeros(test_batch_size,28,device=torch.device(\"cuda\"))\n",
        "  for index in range(test_batch_size):\n",
        "    targets_tensor[index][targets[index]] = 1\n",
        "  return targets_tensor\n",
        "\n",
        "# Get the brute accuracy of our model\n",
        "# This doesn't really do a good job of characterizing the performance as it is the\n",
        "# raw accuracy (which includes predicting 0 versus 1 for each class)\n",
        "def get_acc(output,targets):\n",
        "    # Get the guess of each class\n",
        "    output = torch.round(torch.sigmoid(output))\n",
        "    # Compare guesses\n",
        "    targets = convert_targets_to_hot_encoding_for_test(targets)\n",
        "    diff = targets - output\n",
        "    avg = torch.mean(torch.abs(diff))\n",
        "    return 1 - avg\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n",
            "loss: CrossEntropyLoss()\n",
            "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 28\n",
            "    Root location: asl-alphabet/asl_alphabet_test/asl_alphabet_test/\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               CenterCrop(size=(200, 200))\n",
            "               Resize(size=(120, 120), interpolation=PIL.Image.BILINEAR)\n",
            "               ToTensor()\n",
            "           )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTyihc3FhzYR",
        "colab_type": "code",
        "outputId": "2d6ff6f6-2aaf-4558-9fc0-a504d8587273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def test_loss(run_idx):\n",
        "    # do a pass on the test set\n",
        "    total_test_loss = 0\n",
        "    total_acc_loss = 0\n",
        "    idx = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        # Wrap tensors in Variables\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        test_outputs = net(inputs)\n",
        "        test_loss_size = loss(test_outputs, labels)\n",
        "        total_test_loss += test_loss_size.data.item()\n",
        "        total_acc_loss += get_acc(test_outputs, labels)\n",
        "        if idx >= 100:\n",
        "          break\n",
        "        idx += 1\n",
        "    test_loss_hist.append(total_test_loss / (idx+1))\n",
        "    test_hist_x.append(run_idx)\n",
        "    print(\"Validation loss = {:.4f}\".format(\n",
        "        total_test_loss / (idx+1)))\n",
        "    print(\"Validation Accuracy = {:.4f}\".format(\n",
        "        total_acc_loss / (idx+1)))\n",
        "\n",
        "\n",
        "training_start_time = time.time()\n",
        "# Loop for n_epochs\n",
        "for epoch in range(n_epochs):\n",
        "  running_loss = 0.0\n",
        "  start_time = time.time()\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "    # Get inputs in right form\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "    \n",
        "    # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    # Compute the loss and find the loss with respect to each parameter of the model\n",
        "    loss_size = loss(outputs, labels)\n",
        "    loss_size.backward()\n",
        "    \n",
        "    # Change each parameter with respect to the recently computed loss.\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update statistics\n",
        "    running_loss += loss_size.data.item()\n",
        "    \n",
        "    # Print every 20th batch of an epoch\n",
        "    if (i % print_every) == print_every-1:\n",
        "        print(\"Epoch {}, Iteration {}\\t train_loss: {:.4f} took: {:.4f}s\".format(\n",
        "            epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "        # Reset running loss and time\n",
        "        train_loss_hist.append(running_loss / print_every)\n",
        "        train_hist_x.append(idx)\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "    # Check test set every nth batch\n",
        "    if (i % test_every) == test_every -1:\n",
        "        print(\"index before test_loss:\", idx)\n",
        "        test_loss(idx)\n",
        "        \n",
        "        idx += 1\n",
        "print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 3.3341 took: 1.5189s\n",
            "Epoch 1, Iteration 40\t train_loss: 3.3344 took: 1.1793s\n",
            "Epoch 1, Iteration 60\t train_loss: 3.3315 took: 1.2285s\n",
            "Epoch 1, Iteration 80\t train_loss: 3.3337 took: 1.2825s\n",
            "Epoch 1, Iteration 100\t train_loss: 3.3313 took: 1.2214s\n",
            "Epoch 1, Iteration 120\t train_loss: 3.3318 took: 1.1823s\n",
            "Epoch 1, Iteration 140\t train_loss: 3.3352 took: 1.2264s\n",
            "Epoch 1, Iteration 160\t train_loss: 3.3328 took: 1.2133s\n",
            "Epoch 1, Iteration 180\t train_loss: 3.3346 took: 1.3112s\n",
            "Epoch 1, Iteration 200\t train_loss: 3.3316 took: 1.1969s\n",
            "index before test_loss: 0\n",
            "Validation loss = 2.2247\n",
            "Validation Accuracy = 0.3976\n",
            "Epoch 1, Iteration 220\t train_loss: 3.3319 took: 1.4007s\n",
            "Epoch 1, Iteration 240\t train_loss: 3.3336 took: 1.2373s\n",
            "Epoch 1, Iteration 260\t train_loss: 3.3329 took: 1.2711s\n",
            "Epoch 1, Iteration 280\t train_loss: 3.3342 took: 1.1892s\n",
            "Epoch 1, Iteration 300\t train_loss: 3.3330 took: 1.2243s\n",
            "Epoch 1, Iteration 320\t train_loss: 3.3322 took: 1.2379s\n",
            "Epoch 1, Iteration 340\t train_loss: 3.3304 took: 1.2995s\n",
            "Epoch 1, Iteration 360\t train_loss: 3.3342 took: 1.2283s\n",
            "Epoch 1, Iteration 380\t train_loss: 3.3323 took: 1.2455s\n",
            "Epoch 1, Iteration 400\t train_loss: 3.3351 took: 1.2502s\n",
            "index before test_loss: 1\n",
            "Validation loss = 2.2228\n",
            "Validation Accuracy = 0.3738\n",
            "Epoch 1, Iteration 420\t train_loss: 3.3333 took: 1.3293s\n",
            "Epoch 1, Iteration 440\t train_loss: 3.3328 took: 1.2185s\n",
            "Epoch 1, Iteration 460\t train_loss: 3.3300 took: 1.2289s\n",
            "Epoch 1, Iteration 480\t train_loss: 3.3335 took: 1.2366s\n",
            "Epoch 1, Iteration 500\t train_loss: 3.3354 took: 1.2785s\n",
            "Epoch 1, Iteration 520\t train_loss: 3.3349 took: 1.1751s\n",
            "Epoch 1, Iteration 540\t train_loss: 3.3348 took: 1.2417s\n",
            "Epoch 1, Iteration 560\t train_loss: 3.3307 took: 1.2947s\n",
            "Epoch 1, Iteration 580\t train_loss: 3.3323 took: 1.2367s\n",
            "Epoch 1, Iteration 600\t train_loss: 3.3324 took: 1.2047s\n",
            "index before test_loss: 2\n",
            "Validation loss = 2.2253\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 1, Iteration 620\t train_loss: 3.3327 took: 1.3703s\n",
            "Epoch 1, Iteration 640\t train_loss: 3.3343 took: 1.2235s\n",
            "Epoch 1, Iteration 660\t train_loss: 3.3319 took: 1.2380s\n",
            "Epoch 1, Iteration 680\t train_loss: 3.3323 took: 1.2191s\n",
            "Epoch 1, Iteration 700\t train_loss: 3.3338 took: 1.2375s\n",
            "Epoch 1, Iteration 720\t train_loss: 3.3321 took: 1.3247s\n",
            "Epoch 1, Iteration 740\t train_loss: 3.3343 took: 1.2082s\n",
            "Epoch 1, Iteration 760\t train_loss: 3.3318 took: 1.1824s\n",
            "Epoch 1, Iteration 780\t train_loss: 3.3318 took: 1.2223s\n",
            "Epoch 1, Iteration 800\t train_loss: 3.3334 took: 1.2304s\n",
            "index before test_loss: 3\n",
            "Validation loss = 2.2247\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 1, Iteration 820\t train_loss: 3.3334 took: 1.4618s\n",
            "Epoch 1, Iteration 840\t train_loss: 3.3340 took: 1.1953s\n",
            "Epoch 1, Iteration 860\t train_loss: 3.3305 took: 1.1985s\n",
            "Epoch 1, Iteration 880\t train_loss: 3.3342 took: 1.2707s\n",
            "Epoch 1, Iteration 900\t train_loss: 3.3343 took: 1.2424s\n",
            "Epoch 1, Iteration 920\t train_loss: 3.3348 took: 1.2018s\n",
            "Epoch 1, Iteration 940\t train_loss: 3.3335 took: 1.2673s\n",
            "Epoch 1, Iteration 960\t train_loss: 3.3322 took: 1.2249s\n",
            "Epoch 1, Iteration 980\t train_loss: 3.3321 took: 1.1630s\n",
            "Epoch 1, Iteration 1000\t train_loss: 3.3307 took: 1.2252s\n",
            "index before test_loss: 4\n",
            "Validation loss = 2.2235\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 1, Iteration 1020\t train_loss: 3.3317 took: 1.4234s\n",
            "Epoch 1, Iteration 1040\t train_loss: 3.3325 took: 1.2922s\n",
            "Epoch 1, Iteration 1060\t train_loss: 3.3341 took: 1.1790s\n",
            "Epoch 1, Iteration 1080\t train_loss: 3.3347 took: 1.1864s\n",
            "Epoch 1, Iteration 1100\t train_loss: 3.3340 took: 1.1955s\n",
            "Epoch 1, Iteration 1120\t train_loss: 3.3327 took: 1.2719s\n",
            "Epoch 1, Iteration 1140\t train_loss: 3.3328 took: 1.1843s\n",
            "Epoch 1, Iteration 1160\t train_loss: 3.3310 took: 1.2308s\n",
            "Epoch 1, Iteration 1180\t train_loss: 3.3325 took: 1.2170s\n",
            "Epoch 1, Iteration 1200\t train_loss: 3.3354 took: 1.2510s\n",
            "index before test_loss: 5\n",
            "Validation loss = 2.2216\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 1, Iteration 1220\t train_loss: 3.3331 took: 1.3806s\n",
            "Epoch 1, Iteration 1240\t train_loss: 3.3342 took: 1.2573s\n",
            "Epoch 1, Iteration 1260\t train_loss: 3.3339 took: 1.2230s\n",
            "Epoch 1, Iteration 1280\t train_loss: 3.3325 took: 1.1976s\n",
            "Epoch 1, Iteration 1300\t train_loss: 3.3333 took: 1.2631s\n",
            "Epoch 1, Iteration 1320\t train_loss: 3.3334 took: 1.2498s\n",
            "Epoch 1, Iteration 1340\t train_loss: 3.3332 took: 1.1997s\n",
            "Epoch 1, Iteration 1360\t train_loss: 3.3336 took: 1.2522s\n",
            "Epoch 1, Iteration 1380\t train_loss: 3.3306 took: 1.2722s\n",
            "Epoch 1, Iteration 1400\t train_loss: 3.3324 took: 1.2050s\n",
            "index before test_loss: 6\n",
            "Validation loss = 2.2218\n",
            "Validation Accuracy = 0.3762\n",
            "Epoch 1, Iteration 1420\t train_loss: 3.3351 took: 1.4190s\n",
            "Epoch 1, Iteration 1440\t train_loss: 3.3319 took: 1.2215s\n",
            "Epoch 1, Iteration 1460\t train_loss: 3.3324 took: 1.2238s\n",
            "Epoch 1, Iteration 1480\t train_loss: 3.3328 took: 1.2002s\n",
            "Epoch 1, Iteration 1500\t train_loss: 3.3332 took: 1.2404s\n",
            "Epoch 1, Iteration 1520\t train_loss: 3.3325 took: 1.2674s\n",
            "Epoch 1, Iteration 1540\t train_loss: 3.3333 took: 1.2385s\n",
            "Epoch 1, Iteration 1560\t train_loss: 3.3337 took: 1.2638s\n",
            "Epoch 1, Iteration 1580\t train_loss: 3.3335 took: 1.1928s\n",
            "Epoch 1, Iteration 1600\t train_loss: 3.3330 took: 1.2005s\n",
            "index before test_loss: 7\n",
            "Validation loss = 2.2214\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 1, Iteration 1620\t train_loss: 3.3317 took: 1.4030s\n",
            "Epoch 1, Iteration 1640\t train_loss: 3.3328 took: 1.2982s\n",
            "Epoch 1, Iteration 1660\t train_loss: 3.3334 took: 1.1972s\n",
            "Epoch 1, Iteration 1680\t train_loss: 3.3336 took: 1.1564s\n",
            "Epoch 1, Iteration 1700\t train_loss: 3.3334 took: 1.2757s\n",
            "Epoch 1, Iteration 1720\t train_loss: 3.3344 took: 1.2022s\n",
            "Epoch 1, Iteration 1740\t train_loss: 3.3319 took: 1.2366s\n",
            "Epoch 1, Iteration 1760\t train_loss: 3.3339 took: 1.2565s\n",
            "Epoch 1, Iteration 1780\t train_loss: 3.3319 took: 1.2335s\n",
            "Epoch 1, Iteration 1800\t train_loss: 3.3332 took: 1.2198s\n",
            "index before test_loss: 8\n",
            "Validation loss = 2.2210\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 1, Iteration 1820\t train_loss: 3.3321 took: 1.3918s\n",
            "Epoch 1, Iteration 1840\t train_loss: 3.3324 took: 1.2747s\n",
            "Epoch 1, Iteration 1860\t train_loss: 3.3316 took: 1.2456s\n",
            "Epoch 1, Iteration 1880\t train_loss: 3.3317 took: 1.2647s\n",
            "Epoch 1, Iteration 1900\t train_loss: 3.3353 took: 1.2362s\n",
            "Epoch 1, Iteration 1920\t train_loss: 3.3334 took: 1.1943s\n",
            "Epoch 1, Iteration 1940\t train_loss: 3.3325 took: 1.2799s\n",
            "Epoch 1, Iteration 1960\t train_loss: 3.3333 took: 1.1469s\n",
            "Epoch 1, Iteration 1980\t train_loss: 3.3333 took: 1.2752s\n",
            "Epoch 1, Iteration 2000\t train_loss: 3.3345 took: 1.1817s\n",
            "index before test_loss: 9\n",
            "Validation loss = 2.2183\n",
            "Validation Accuracy = 0.4905\n",
            "Epoch 1, Iteration 2020\t train_loss: 3.3337 took: 1.5165s\n",
            "Epoch 1, Iteration 2040\t train_loss: 3.3321 took: 1.1745s\n",
            "Epoch 1, Iteration 2060\t train_loss: 3.3337 took: 1.2445s\n",
            "Epoch 1, Iteration 2080\t train_loss: 3.3348 took: 1.2518s\n",
            "Epoch 1, Iteration 2100\t train_loss: 3.3348 took: 1.2167s\n",
            "Epoch 1, Iteration 2120\t train_loss: 3.3324 took: 1.2127s\n",
            "Epoch 1, Iteration 2140\t train_loss: 3.3325 took: 1.2273s\n",
            "Epoch 1, Iteration 2160\t train_loss: 3.3327 took: 1.2112s\n",
            "Epoch 1, Iteration 2180\t train_loss: 3.3316 took: 1.2443s\n",
            "Epoch 1, Iteration 2200\t train_loss: 3.3321 took: 1.2578s\n",
            "index before test_loss: 10\n",
            "Validation loss = 2.2222\n",
            "Validation Accuracy = 0.3976\n",
            "Epoch 1, Iteration 2220\t train_loss: 3.3346 took: 1.3713s\n",
            "Epoch 1, Iteration 2240\t train_loss: 3.3333 took: 1.1990s\n",
            "Epoch 1, Iteration 2260\t train_loss: 3.3326 took: 1.1791s\n",
            "Epoch 1, Iteration 2280\t train_loss: 3.3309 took: 1.2985s\n",
            "Epoch 1, Iteration 2300\t train_loss: 3.3336 took: 1.2484s\n",
            "Epoch 1, Iteration 2320\t train_loss: 3.3331 took: 1.2404s\n",
            "Epoch 1, Iteration 2340\t train_loss: 3.3351 took: 1.1877s\n",
            "Epoch 1, Iteration 2360\t train_loss: 3.3310 took: 1.2234s\n",
            "Epoch 1, Iteration 2380\t train_loss: 3.3330 took: 1.2365s\n",
            "Epoch 1, Iteration 2400\t train_loss: 3.3320 took: 1.2237s\n",
            "index before test_loss: 11\n",
            "Validation loss = 2.2205\n",
            "Validation Accuracy = 0.5119\n",
            "Epoch 1, Iteration 2420\t train_loss: 3.3355 took: 1.4192s\n",
            "Epoch 1, Iteration 2440\t train_loss: 3.3339 took: 1.1950s\n",
            "Epoch 1, Iteration 2460\t train_loss: 3.3328 took: 1.2156s\n",
            "Epoch 1, Iteration 2480\t train_loss: 3.3311 took: 1.3091s\n",
            "Epoch 1, Iteration 2500\t train_loss: 3.3332 took: 1.1709s\n",
            "Epoch 1, Iteration 2520\t train_loss: 3.3347 took: 1.2296s\n",
            "Epoch 1, Iteration 2540\t train_loss: 3.3339 took: 1.2234s\n",
            "Epoch 1, Iteration 2560\t train_loss: 3.3307 took: 1.2632s\n",
            "Epoch 1, Iteration 2580\t train_loss: 3.3330 took: 1.1985s\n",
            "Epoch 1, Iteration 2600\t train_loss: 3.3348 took: 1.2223s\n",
            "index before test_loss: 12\n",
            "Validation loss = 2.2195\n",
            "Validation Accuracy = 0.4690\n",
            "Epoch 1, Iteration 2620\t train_loss: 3.3307 took: 1.3872s\n",
            "Epoch 2, Iteration 20\t train_loss: 3.3329 took: 1.3954s\n",
            "Epoch 2, Iteration 40\t train_loss: 3.3327 took: 1.2071s\n",
            "Epoch 2, Iteration 60\t train_loss: 3.3326 took: 1.2825s\n",
            "Epoch 2, Iteration 80\t train_loss: 3.3309 took: 1.3146s\n",
            "Epoch 2, Iteration 100\t train_loss: 3.3321 took: 1.1622s\n",
            "Epoch 2, Iteration 120\t train_loss: 3.3329 took: 1.2207s\n",
            "Epoch 2, Iteration 140\t train_loss: 3.3335 took: 1.3007s\n",
            "Epoch 2, Iteration 160\t train_loss: 3.3317 took: 1.2073s\n",
            "Epoch 2, Iteration 180\t train_loss: 3.3327 took: 1.2123s\n",
            "Epoch 2, Iteration 200\t train_loss: 3.3326 took: 1.2479s\n",
            "index before test_loss: 13\n",
            "Validation loss = 2.2179\n",
            "Validation Accuracy = 0.4024\n",
            "Epoch 2, Iteration 220\t train_loss: 3.3326 took: 1.4062s\n",
            "Epoch 2, Iteration 240\t train_loss: 3.3356 took: 1.2361s\n",
            "Epoch 2, Iteration 260\t train_loss: 3.3336 took: 1.2110s\n",
            "Epoch 2, Iteration 280\t train_loss: 3.3311 took: 1.2646s\n",
            "Epoch 2, Iteration 300\t train_loss: 3.3306 took: 1.2237s\n",
            "Epoch 2, Iteration 320\t train_loss: 3.3357 took: 1.2245s\n",
            "Epoch 2, Iteration 340\t train_loss: 3.3335 took: 1.2044s\n",
            "Epoch 2, Iteration 360\t train_loss: 3.3368 took: 1.2174s\n",
            "Epoch 2, Iteration 380\t train_loss: 3.3350 took: 1.2492s\n",
            "Epoch 2, Iteration 400\t train_loss: 3.3316 took: 1.1867s\n",
            "index before test_loss: 14\n",
            "Validation loss = 2.2193\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 2, Iteration 420\t train_loss: 3.3319 took: 1.3715s\n",
            "Epoch 2, Iteration 440\t train_loss: 3.3344 took: 1.2688s\n",
            "Epoch 2, Iteration 460\t train_loss: 3.3343 took: 1.1920s\n",
            "Epoch 2, Iteration 480\t train_loss: 3.3314 took: 1.2606s\n",
            "Epoch 2, Iteration 500\t train_loss: 3.3348 took: 1.1932s\n",
            "Epoch 2, Iteration 520\t train_loss: 3.3310 took: 1.2274s\n",
            "Epoch 2, Iteration 540\t train_loss: 3.3336 took: 1.1953s\n",
            "Epoch 2, Iteration 560\t train_loss: 3.3320 took: 1.2477s\n",
            "Epoch 2, Iteration 580\t train_loss: 3.3336 took: 1.2387s\n",
            "Epoch 2, Iteration 600\t train_loss: 3.3331 took: 1.1970s\n",
            "index before test_loss: 15\n",
            "Validation loss = 2.2215\n",
            "Validation Accuracy = 0.4476\n",
            "Epoch 2, Iteration 620\t train_loss: 3.3308 took: 1.3954s\n",
            "Epoch 2, Iteration 640\t train_loss: 3.3334 took: 1.2603s\n",
            "Epoch 2, Iteration 660\t train_loss: 3.3349 took: 1.2321s\n",
            "Epoch 2, Iteration 680\t train_loss: 3.3340 took: 1.1930s\n",
            "Epoch 2, Iteration 700\t train_loss: 3.3329 took: 1.2164s\n",
            "Epoch 2, Iteration 720\t train_loss: 3.3314 took: 1.2590s\n",
            "Epoch 2, Iteration 740\t train_loss: 3.3322 took: 1.1861s\n",
            "Epoch 2, Iteration 760\t train_loss: 3.3315 took: 1.2383s\n",
            "Epoch 2, Iteration 780\t train_loss: 3.3311 took: 1.2796s\n",
            "Epoch 2, Iteration 800\t train_loss: 3.3316 took: 1.2175s\n",
            "index before test_loss: 16\n",
            "Validation loss = 2.2192\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 2, Iteration 820\t train_loss: 3.3355 took: 1.3815s\n",
            "Epoch 2, Iteration 840\t train_loss: 3.3329 took: 1.2148s\n",
            "Epoch 2, Iteration 860\t train_loss: 3.3350 took: 1.2314s\n",
            "Epoch 2, Iteration 880\t train_loss: 3.3331 took: 1.2471s\n",
            "Epoch 2, Iteration 900\t train_loss: 3.3310 took: 1.1837s\n",
            "Epoch 2, Iteration 920\t train_loss: 3.3332 took: 1.2907s\n",
            "Epoch 2, Iteration 940\t train_loss: 3.3351 took: 1.1818s\n",
            "Epoch 2, Iteration 960\t train_loss: 3.3313 took: 1.2167s\n",
            "Epoch 2, Iteration 980\t train_loss: 3.3317 took: 1.1996s\n",
            "Epoch 2, Iteration 1000\t train_loss: 3.3326 took: 1.2045s\n",
            "index before test_loss: 17\n",
            "Validation loss = 2.2204\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 2, Iteration 1020\t train_loss: 3.3297 took: 1.3873s\n",
            "Epoch 2, Iteration 1040\t train_loss: 3.3344 took: 1.2548s\n",
            "Epoch 2, Iteration 1060\t train_loss: 3.3329 took: 1.2436s\n",
            "Epoch 2, Iteration 1080\t train_loss: 3.3310 took: 1.1674s\n",
            "Epoch 2, Iteration 1100\t train_loss: 3.3322 took: 1.2835s\n",
            "Epoch 2, Iteration 1120\t train_loss: 3.3343 took: 1.2204s\n",
            "Epoch 2, Iteration 1140\t train_loss: 3.3340 took: 1.2225s\n",
            "Epoch 2, Iteration 1160\t train_loss: 3.3305 took: 1.2481s\n",
            "Epoch 2, Iteration 1180\t train_loss: 3.3328 took: 1.2073s\n",
            "Epoch 2, Iteration 1200\t train_loss: 3.3363 took: 1.2687s\n",
            "index before test_loss: 18\n",
            "Validation loss = 2.2221\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 2, Iteration 1220\t train_loss: 3.3321 took: 1.3359s\n",
            "Epoch 2, Iteration 1240\t train_loss: 3.3348 took: 1.1859s\n",
            "Epoch 2, Iteration 1260\t train_loss: 3.3307 took: 1.2748s\n",
            "Epoch 2, Iteration 1280\t train_loss: 3.3325 took: 1.2436s\n",
            "Epoch 2, Iteration 1300\t train_loss: 3.3337 took: 1.2419s\n",
            "Epoch 2, Iteration 1320\t train_loss: 3.3359 took: 1.2324s\n",
            "Epoch 2, Iteration 1340\t train_loss: 3.3318 took: 1.2192s\n",
            "Epoch 2, Iteration 1360\t train_loss: 3.3338 took: 1.2246s\n",
            "Epoch 2, Iteration 1380\t train_loss: 3.3312 took: 1.2284s\n",
            "Epoch 2, Iteration 1400\t train_loss: 3.3343 took: 1.2577s\n",
            "index before test_loss: 19\n",
            "Validation loss = 2.2256\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 2, Iteration 1420\t train_loss: 3.3353 took: 1.3585s\n",
            "Epoch 2, Iteration 1440\t train_loss: 3.3352 took: 1.2402s\n",
            "Epoch 2, Iteration 1460\t train_loss: 3.3312 took: 1.2217s\n",
            "Epoch 2, Iteration 1480\t train_loss: 3.3328 took: 1.2244s\n",
            "Epoch 2, Iteration 1500\t train_loss: 3.3345 took: 1.2194s\n",
            "Epoch 2, Iteration 1520\t train_loss: 3.3314 took: 1.2933s\n",
            "Epoch 2, Iteration 1540\t train_loss: 3.3308 took: 1.2174s\n",
            "Epoch 2, Iteration 1560\t train_loss: 3.3317 took: 1.2953s\n",
            "Epoch 2, Iteration 1580\t train_loss: 3.3352 took: 1.2256s\n",
            "Epoch 2, Iteration 1600\t train_loss: 3.3335 took: 1.1814s\n",
            "index before test_loss: 20\n",
            "Validation loss = 2.2196\n",
            "Validation Accuracy = 0.3786\n",
            "Epoch 2, Iteration 1620\t train_loss: 3.3322 took: 1.3994s\n",
            "Epoch 2, Iteration 1640\t train_loss: 3.3317 took: 1.2713s\n",
            "Epoch 2, Iteration 1660\t train_loss: 3.3338 took: 1.2059s\n",
            "Epoch 2, Iteration 1680\t train_loss: 3.3339 took: 1.2456s\n",
            "Epoch 2, Iteration 1700\t train_loss: 3.3314 took: 1.2865s\n",
            "Epoch 2, Iteration 1720\t train_loss: 3.3344 took: 1.2174s\n",
            "Epoch 2, Iteration 1740\t train_loss: 3.3337 took: 1.1959s\n",
            "Epoch 2, Iteration 1760\t train_loss: 3.3332 took: 1.2863s\n",
            "Epoch 2, Iteration 1780\t train_loss: 3.3318 took: 1.1954s\n",
            "Epoch 2, Iteration 1800\t train_loss: 3.3314 took: 1.1894s\n",
            "index before test_loss: 21\n",
            "Validation loss = 2.2185\n",
            "Validation Accuracy = 0.4929\n",
            "Epoch 2, Iteration 1820\t train_loss: 3.3322 took: 1.3647s\n",
            "Epoch 2, Iteration 1840\t train_loss: 3.3309 took: 1.2538s\n",
            "Epoch 2, Iteration 1860\t train_loss: 3.3319 took: 1.2396s\n",
            "Epoch 2, Iteration 1880\t train_loss: 3.3350 took: 1.2227s\n",
            "Epoch 2, Iteration 1900\t train_loss: 3.3341 took: 1.1671s\n",
            "Epoch 2, Iteration 1920\t train_loss: 3.3361 took: 1.2465s\n",
            "Epoch 2, Iteration 1940\t train_loss: 3.3317 took: 1.2318s\n",
            "Epoch 2, Iteration 1960\t train_loss: 3.3354 took: 1.2237s\n",
            "Epoch 2, Iteration 1980\t train_loss: 3.3341 took: 1.2406s\n",
            "Epoch 2, Iteration 2000\t train_loss: 3.3323 took: 1.2286s\n",
            "index before test_loss: 22\n",
            "Validation loss = 2.2230\n",
            "Validation Accuracy = 0.4405\n",
            "Epoch 2, Iteration 2020\t train_loss: 3.3349 took: 1.4393s\n",
            "Epoch 2, Iteration 2040\t train_loss: 3.3347 took: 1.2343s\n",
            "Epoch 2, Iteration 2060\t train_loss: 3.3335 took: 1.1844s\n",
            "Epoch 2, Iteration 2080\t train_loss: 3.3314 took: 1.2195s\n",
            "Epoch 2, Iteration 2100\t train_loss: 3.3312 took: 1.2195s\n",
            "Epoch 2, Iteration 2120\t train_loss: 3.3325 took: 1.2284s\n",
            "Epoch 2, Iteration 2140\t train_loss: 3.3314 took: 1.2643s\n",
            "Epoch 2, Iteration 2160\t train_loss: 3.3342 took: 1.2538s\n",
            "Epoch 2, Iteration 2180\t train_loss: 3.3331 took: 1.1968s\n",
            "Epoch 2, Iteration 2200\t train_loss: 3.3316 took: 1.2339s\n",
            "index before test_loss: 23\n",
            "Validation loss = 2.2266\n",
            "Validation Accuracy = 0.4405\n",
            "Epoch 2, Iteration 2220\t train_loss: 3.3326 took: 1.3503s\n",
            "Epoch 2, Iteration 2240\t train_loss: 3.3333 took: 1.2347s\n",
            "Epoch 2, Iteration 2260\t train_loss: 3.3355 took: 1.2824s\n",
            "Epoch 2, Iteration 2280\t train_loss: 3.3343 took: 1.2406s\n",
            "Epoch 2, Iteration 2300\t train_loss: 3.3326 took: 1.2060s\n",
            "Epoch 2, Iteration 2320\t train_loss: 3.3344 took: 1.2313s\n",
            "Epoch 2, Iteration 2340\t train_loss: 3.3314 took: 1.2936s\n",
            "Epoch 2, Iteration 2360\t train_loss: 3.3337 took: 1.1775s\n",
            "Epoch 2, Iteration 2380\t train_loss: 3.3334 took: 1.2070s\n",
            "Epoch 2, Iteration 2400\t train_loss: 3.3329 took: 1.2025s\n",
            "index before test_loss: 24\n",
            "Validation loss = 2.2255\n",
            "Validation Accuracy = 0.4595\n",
            "Epoch 2, Iteration 2420\t train_loss: 3.3316 took: 1.3852s\n",
            "Epoch 2, Iteration 2440\t train_loss: 3.3342 took: 1.2012s\n",
            "Epoch 2, Iteration 2460\t train_loss: 3.3312 took: 1.2300s\n",
            "Epoch 2, Iteration 2480\t train_loss: 3.3311 took: 1.2111s\n",
            "Epoch 2, Iteration 2500\t train_loss: 3.3353 took: 1.1842s\n",
            "Epoch 2, Iteration 2520\t train_loss: 3.3341 took: 1.2479s\n",
            "Epoch 2, Iteration 2540\t train_loss: 3.3328 took: 1.2183s\n",
            "Epoch 2, Iteration 2560\t train_loss: 3.3334 took: 1.2787s\n",
            "Epoch 2, Iteration 2580\t train_loss: 3.3315 took: 1.2052s\n",
            "Epoch 2, Iteration 2600\t train_loss: 3.3329 took: 1.2224s\n",
            "index before test_loss: 25\n",
            "Validation loss = 2.2239\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 2, Iteration 2620\t train_loss: 3.3322 took: 1.3814s\n",
            "Epoch 3, Iteration 20\t train_loss: 3.3347 took: 1.3606s\n",
            "Epoch 3, Iteration 40\t train_loss: 3.3320 took: 1.2770s\n",
            "Epoch 3, Iteration 60\t train_loss: 3.3326 took: 1.1644s\n",
            "Epoch 3, Iteration 80\t train_loss: 3.3315 took: 1.3634s\n",
            "Epoch 3, Iteration 100\t train_loss: 3.3321 took: 1.2745s\n",
            "Epoch 3, Iteration 120\t train_loss: 3.3325 took: 1.2174s\n",
            "Epoch 3, Iteration 140\t train_loss: 3.3319 took: 1.2101s\n",
            "Epoch 3, Iteration 160\t train_loss: 3.3332 took: 1.1524s\n",
            "Epoch 3, Iteration 180\t train_loss: 3.3335 took: 1.2423s\n",
            "Epoch 3, Iteration 200\t train_loss: 3.3329 took: 1.1896s\n",
            "index before test_loss: 26\n",
            "Validation loss = 2.2195\n",
            "Validation Accuracy = 0.4024\n",
            "Epoch 3, Iteration 220\t train_loss: 3.3298 took: 1.4601s\n",
            "Epoch 3, Iteration 240\t train_loss: 3.3317 took: 1.1948s\n",
            "Epoch 3, Iteration 260\t train_loss: 3.3326 took: 1.2024s\n",
            "Epoch 3, Iteration 280\t train_loss: 3.3319 took: 1.2031s\n",
            "Epoch 3, Iteration 300\t train_loss: 3.3354 took: 1.2130s\n",
            "Epoch 3, Iteration 320\t train_loss: 3.3354 took: 1.2780s\n",
            "Epoch 3, Iteration 340\t train_loss: 3.3343 took: 1.1706s\n",
            "Epoch 3, Iteration 360\t train_loss: 3.3330 took: 1.2492s\n",
            "Epoch 3, Iteration 380\t train_loss: 3.3339 took: 1.2737s\n",
            "Epoch 3, Iteration 400\t train_loss: 3.3327 took: 1.2069s\n",
            "index before test_loss: 27\n",
            "Validation loss = 2.2232\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 3, Iteration 420\t train_loss: 3.3315 took: 1.3934s\n",
            "Epoch 3, Iteration 440\t train_loss: 3.3336 took: 1.1996s\n",
            "Epoch 3, Iteration 460\t train_loss: 3.3335 took: 1.2732s\n",
            "Epoch 3, Iteration 480\t train_loss: 3.3342 took: 1.2023s\n",
            "Epoch 3, Iteration 500\t train_loss: 3.3327 took: 1.2833s\n",
            "Epoch 3, Iteration 520\t train_loss: 3.3334 took: 1.2122s\n",
            "Epoch 3, Iteration 540\t train_loss: 3.3342 took: 1.2827s\n",
            "Epoch 3, Iteration 560\t train_loss: 3.3327 took: 1.1671s\n",
            "Epoch 3, Iteration 580\t train_loss: 3.3338 took: 1.2021s\n",
            "Epoch 3, Iteration 600\t train_loss: 3.3330 took: 1.2477s\n",
            "index before test_loss: 28\n",
            "Validation loss = 2.2249\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 3, Iteration 620\t train_loss: 3.3322 took: 1.3213s\n",
            "Epoch 3, Iteration 640\t train_loss: 3.3318 took: 1.2701s\n",
            "Epoch 3, Iteration 660\t train_loss: 3.3354 took: 1.1966s\n",
            "Epoch 3, Iteration 680\t train_loss: 3.3327 took: 1.2623s\n",
            "Epoch 3, Iteration 700\t train_loss: 3.3323 took: 1.2422s\n",
            "Epoch 3, Iteration 720\t train_loss: 3.3311 took: 1.2223s\n",
            "Epoch 3, Iteration 740\t train_loss: 3.3344 took: 1.2149s\n",
            "Epoch 3, Iteration 760\t train_loss: 3.3331 took: 1.1998s\n",
            "Epoch 3, Iteration 780\t train_loss: 3.3316 took: 1.1873s\n",
            "Epoch 3, Iteration 800\t train_loss: 3.3345 took: 1.1995s\n",
            "index before test_loss: 29\n",
            "Validation loss = 2.2225\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 3, Iteration 820\t train_loss: 3.3336 took: 1.3703s\n",
            "Epoch 3, Iteration 840\t train_loss: 3.3326 took: 1.2130s\n",
            "Epoch 3, Iteration 860\t train_loss: 3.3334 took: 1.2218s\n",
            "Epoch 3, Iteration 880\t train_loss: 3.3344 took: 1.2349s\n",
            "Epoch 3, Iteration 900\t train_loss: 3.3338 took: 1.2128s\n",
            "Epoch 3, Iteration 920\t train_loss: 3.3340 took: 1.2314s\n",
            "Epoch 3, Iteration 940\t train_loss: 3.3323 took: 1.2631s\n",
            "Epoch 3, Iteration 960\t train_loss: 3.3311 took: 1.2081s\n",
            "Epoch 3, Iteration 980\t train_loss: 3.3345 took: 1.2345s\n",
            "Epoch 3, Iteration 1000\t train_loss: 3.3316 took: 1.1523s\n",
            "index before test_loss: 30\n",
            "Validation loss = 2.2202\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 3, Iteration 1020\t train_loss: 3.3339 took: 1.4391s\n",
            "Epoch 3, Iteration 1040\t train_loss: 3.3315 took: 1.2470s\n",
            "Epoch 3, Iteration 1060\t train_loss: 3.3330 took: 1.2179s\n",
            "Epoch 3, Iteration 1080\t train_loss: 3.3351 took: 1.1933s\n",
            "Epoch 3, Iteration 1100\t train_loss: 3.3317 took: 1.2675s\n",
            "Epoch 3, Iteration 1120\t train_loss: 3.3333 took: 1.2164s\n",
            "Epoch 3, Iteration 1140\t train_loss: 3.3339 took: 1.1837s\n",
            "Epoch 3, Iteration 1160\t train_loss: 3.3331 took: 1.2221s\n",
            "Epoch 3, Iteration 1180\t train_loss: 3.3345 took: 1.2558s\n",
            "Epoch 3, Iteration 1200\t train_loss: 3.3317 took: 1.2400s\n",
            "index before test_loss: 31\n",
            "Validation loss = 2.2158\n",
            "Validation Accuracy = 0.4500\n",
            "Epoch 3, Iteration 1220\t train_loss: 3.3342 took: 1.4198s\n",
            "Epoch 3, Iteration 1240\t train_loss: 3.3334 took: 1.1640s\n",
            "Epoch 3, Iteration 1260\t train_loss: 3.3339 took: 1.2661s\n",
            "Epoch 3, Iteration 1280\t train_loss: 3.3326 took: 1.2376s\n",
            "Epoch 3, Iteration 1300\t train_loss: 3.3323 took: 1.2381s\n",
            "Epoch 3, Iteration 1320\t train_loss: 3.3312 took: 1.2312s\n",
            "Epoch 3, Iteration 1340\t train_loss: 3.3350 took: 1.2530s\n",
            "Epoch 3, Iteration 1360\t train_loss: 3.3307 took: 1.2028s\n",
            "Epoch 3, Iteration 1380\t train_loss: 3.3314 took: 1.2374s\n",
            "Epoch 3, Iteration 1400\t train_loss: 3.3320 took: 1.2672s\n",
            "index before test_loss: 32\n",
            "Validation loss = 2.2185\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 3, Iteration 1420\t train_loss: 3.3326 took: 1.3490s\n",
            "Epoch 3, Iteration 1440\t train_loss: 3.3298 took: 1.2476s\n",
            "Epoch 3, Iteration 1460\t train_loss: 3.3348 took: 1.2097s\n",
            "Epoch 3, Iteration 1480\t train_loss: 3.3355 took: 1.2280s\n",
            "Epoch 3, Iteration 1500\t train_loss: 3.3296 took: 1.2404s\n",
            "Epoch 3, Iteration 1520\t train_loss: 3.3331 took: 1.2181s\n",
            "Epoch 3, Iteration 1540\t train_loss: 3.3318 took: 1.2966s\n",
            "Epoch 3, Iteration 1560\t train_loss: 3.3347 took: 1.2000s\n",
            "Epoch 3, Iteration 1580\t train_loss: 3.3339 took: 1.2931s\n",
            "Epoch 3, Iteration 1600\t train_loss: 3.3355 took: 1.1724s\n",
            "index before test_loss: 33\n",
            "Validation loss = 2.2219\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 3, Iteration 1620\t train_loss: 3.3332 took: 1.4097s\n",
            "Epoch 3, Iteration 1640\t train_loss: 3.3333 took: 1.2433s\n",
            "Epoch 3, Iteration 1660\t train_loss: 3.3320 took: 1.2377s\n",
            "Epoch 3, Iteration 1680\t train_loss: 3.3320 took: 1.2504s\n",
            "Epoch 3, Iteration 1700\t train_loss: 3.3344 took: 1.2604s\n",
            "Epoch 3, Iteration 1720\t train_loss: 3.3349 took: 1.1944s\n",
            "Epoch 3, Iteration 1740\t train_loss: 3.3316 took: 1.2260s\n",
            "Epoch 3, Iteration 1760\t train_loss: 3.3313 took: 1.2417s\n",
            "Epoch 3, Iteration 1780\t train_loss: 3.3314 took: 1.2034s\n",
            "Epoch 3, Iteration 1800\t train_loss: 3.3347 took: 1.3085s\n",
            "index before test_loss: 34\n",
            "Validation loss = 2.2226\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 3, Iteration 1820\t train_loss: 3.3338 took: 1.2919s\n",
            "Epoch 3, Iteration 1840\t train_loss: 3.3321 took: 1.2586s\n",
            "Epoch 3, Iteration 1860\t train_loss: 3.3332 took: 1.2480s\n",
            "Epoch 3, Iteration 1880\t train_loss: 3.3336 took: 1.2244s\n",
            "Epoch 3, Iteration 1900\t train_loss: 3.3347 took: 1.2693s\n",
            "Epoch 3, Iteration 1920\t train_loss: 3.3314 took: 1.2575s\n",
            "Epoch 3, Iteration 1940\t train_loss: 3.3323 took: 1.1821s\n",
            "Epoch 3, Iteration 1960\t train_loss: 3.3296 took: 1.2542s\n",
            "Epoch 3, Iteration 1980\t train_loss: 3.3307 took: 1.2483s\n",
            "Epoch 3, Iteration 2000\t train_loss: 3.3357 took: 1.2395s\n",
            "index before test_loss: 35\n",
            "Validation loss = 2.2200\n",
            "Validation Accuracy = 0.4262\n",
            "Epoch 3, Iteration 2020\t train_loss: 3.3298 took: 1.4067s\n",
            "Epoch 3, Iteration 2040\t train_loss: 3.3328 took: 1.2286s\n",
            "Epoch 3, Iteration 2060\t train_loss: 3.3334 took: 1.2249s\n",
            "Epoch 3, Iteration 2080\t train_loss: 3.3329 took: 1.2720s\n",
            "Epoch 3, Iteration 2100\t train_loss: 3.3336 took: 1.1630s\n",
            "Epoch 3, Iteration 2120\t train_loss: 3.3318 took: 1.2585s\n",
            "Epoch 3, Iteration 2140\t train_loss: 3.3345 took: 1.2196s\n",
            "Epoch 3, Iteration 2160\t train_loss: 3.3320 took: 1.2404s\n",
            "Epoch 3, Iteration 2180\t train_loss: 3.3378 took: 1.2353s\n",
            "Epoch 3, Iteration 2200\t train_loss: 3.3347 took: 1.2711s\n",
            "index before test_loss: 36\n",
            "Validation loss = 2.2231\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 3, Iteration 2220\t train_loss: 3.3327 took: 1.3282s\n",
            "Epoch 3, Iteration 2240\t train_loss: 3.3339 took: 1.2407s\n",
            "Epoch 3, Iteration 2260\t train_loss: 3.3329 took: 1.2594s\n",
            "Epoch 3, Iteration 2280\t train_loss: 3.3327 took: 1.2256s\n",
            "Epoch 3, Iteration 2300\t train_loss: 3.3311 took: 1.2039s\n",
            "Epoch 3, Iteration 2320\t train_loss: 3.3343 took: 1.2325s\n",
            "Epoch 3, Iteration 2340\t train_loss: 3.3326 took: 1.2323s\n",
            "Epoch 3, Iteration 2360\t train_loss: 3.3329 took: 1.2070s\n",
            "Epoch 3, Iteration 2380\t train_loss: 3.3333 took: 1.2872s\n",
            "Epoch 3, Iteration 2400\t train_loss: 3.3317 took: 1.2414s\n",
            "index before test_loss: 37\n",
            "Validation loss = 2.2251\n",
            "Validation Accuracy = 0.3952\n",
            "Epoch 3, Iteration 2420\t train_loss: 3.3324 took: 1.3726s\n",
            "Epoch 3, Iteration 2440\t train_loss: 3.3335 took: 1.1848s\n",
            "Epoch 3, Iteration 2460\t train_loss: 3.3339 took: 1.2253s\n",
            "Epoch 3, Iteration 2480\t train_loss: 3.3340 took: 1.2374s\n",
            "Epoch 3, Iteration 2500\t train_loss: 3.3343 took: 1.2542s\n",
            "Epoch 3, Iteration 2520\t train_loss: 3.3328 took: 1.2445s\n",
            "Epoch 3, Iteration 2540\t train_loss: 3.3321 took: 1.2623s\n",
            "Epoch 3, Iteration 2560\t train_loss: 3.3333 took: 1.1967s\n",
            "Epoch 3, Iteration 2580\t train_loss: 3.3335 took: 1.2693s\n",
            "Epoch 3, Iteration 2600\t train_loss: 3.3342 took: 1.1858s\n",
            "index before test_loss: 38\n",
            "Validation loss = 2.2220\n",
            "Validation Accuracy = 0.4214\n",
            "Epoch 3, Iteration 2620\t train_loss: 3.3306 took: 1.3555s\n",
            "Epoch 4, Iteration 20\t train_loss: 3.3333 took: 1.3428s\n",
            "Epoch 4, Iteration 40\t train_loss: 3.3330 took: 1.2220s\n",
            "Epoch 4, Iteration 60\t train_loss: 3.3304 took: 1.2977s\n",
            "Epoch 4, Iteration 80\t train_loss: 3.3344 took: 1.1837s\n",
            "Epoch 4, Iteration 100\t train_loss: 3.3323 took: 1.2115s\n",
            "Epoch 4, Iteration 120\t train_loss: 3.3320 took: 1.2581s\n",
            "Epoch 4, Iteration 140\t train_loss: 3.3318 took: 1.2478s\n",
            "Epoch 4, Iteration 160\t train_loss: 3.3317 took: 1.2084s\n",
            "Epoch 4, Iteration 180\t train_loss: 3.3357 took: 1.2096s\n",
            "Epoch 4, Iteration 200\t train_loss: 3.3313 took: 1.2701s\n",
            "index before test_loss: 39\n",
            "Validation loss = 2.2174\n",
            "Validation Accuracy = 0.4476\n",
            "Epoch 4, Iteration 220\t train_loss: 3.3342 took: 1.3627s\n",
            "Epoch 4, Iteration 240\t train_loss: 3.3338 took: 1.2605s\n",
            "Epoch 4, Iteration 260\t train_loss: 3.3330 took: 1.2225s\n",
            "Epoch 4, Iteration 280\t train_loss: 3.3322 took: 1.2489s\n",
            "Epoch 4, Iteration 300\t train_loss: 3.3346 took: 1.1950s\n",
            "Epoch 4, Iteration 320\t train_loss: 3.3336 took: 1.1947s\n",
            "Epoch 4, Iteration 340\t train_loss: 3.3347 took: 1.2384s\n",
            "Epoch 4, Iteration 360\t train_loss: 3.3324 took: 1.2565s\n",
            "Epoch 4, Iteration 380\t train_loss: 3.3334 took: 1.1760s\n",
            "Epoch 4, Iteration 400\t train_loss: 3.3324 took: 1.2397s\n",
            "index before test_loss: 40\n",
            "Validation loss = 2.2233\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 4, Iteration 420\t train_loss: 3.3345 took: 1.3601s\n",
            "Epoch 4, Iteration 440\t train_loss: 3.3330 took: 1.2218s\n",
            "Epoch 4, Iteration 460\t train_loss: 3.3345 took: 1.2344s\n",
            "Epoch 4, Iteration 480\t train_loss: 3.3322 took: 1.2410s\n",
            "Epoch 4, Iteration 500\t train_loss: 3.3321 took: 1.1698s\n",
            "Epoch 4, Iteration 520\t train_loss: 3.3329 took: 1.2061s\n",
            "Epoch 4, Iteration 540\t train_loss: 3.3303 took: 1.2313s\n",
            "Epoch 4, Iteration 560\t train_loss: 3.3328 took: 1.2154s\n",
            "Epoch 4, Iteration 580\t train_loss: 3.3316 took: 1.2876s\n",
            "Epoch 4, Iteration 600\t train_loss: 3.3344 took: 1.2132s\n",
            "index before test_loss: 41\n",
            "Validation loss = 2.2191\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 4, Iteration 620\t train_loss: 3.3309 took: 1.3548s\n",
            "Epoch 4, Iteration 640\t train_loss: 3.3359 took: 1.2162s\n",
            "Epoch 4, Iteration 660\t train_loss: 3.3327 took: 1.2034s\n",
            "Epoch 4, Iteration 680\t train_loss: 3.3312 took: 1.2434s\n",
            "Epoch 4, Iteration 700\t train_loss: 3.3313 took: 1.2120s\n",
            "Epoch 4, Iteration 720\t train_loss: 3.3353 took: 1.2464s\n",
            "Epoch 4, Iteration 740\t train_loss: 3.3344 took: 1.2277s\n",
            "Epoch 4, Iteration 760\t train_loss: 3.3341 took: 1.2046s\n",
            "Epoch 4, Iteration 780\t train_loss: 3.3310 took: 1.1818s\n",
            "Epoch 4, Iteration 800\t train_loss: 3.3324 took: 1.2590s\n",
            "index before test_loss: 42\n",
            "Validation loss = 2.2226\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 4, Iteration 820\t train_loss: 3.3338 took: 1.3632s\n",
            "Epoch 4, Iteration 840\t train_loss: 3.3309 took: 1.2014s\n",
            "Epoch 4, Iteration 860\t train_loss: 3.3354 took: 1.1998s\n",
            "Epoch 4, Iteration 880\t train_loss: 3.3345 took: 1.2412s\n",
            "Epoch 4, Iteration 900\t train_loss: 3.3326 took: 1.2638s\n",
            "Epoch 4, Iteration 920\t train_loss: 3.3337 took: 1.2158s\n",
            "Epoch 4, Iteration 940\t train_loss: 3.3333 took: 1.2750s\n",
            "Epoch 4, Iteration 960\t train_loss: 3.3318 took: 1.2075s\n",
            "Epoch 4, Iteration 980\t train_loss: 3.3322 took: 1.2186s\n",
            "Epoch 4, Iteration 1000\t train_loss: 3.3338 took: 1.2247s\n",
            "index before test_loss: 43\n",
            "Validation loss = 2.2247\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 4, Iteration 1020\t train_loss: 3.3314 took: 1.4003s\n",
            "Epoch 4, Iteration 1040\t train_loss: 3.3317 took: 1.2748s\n",
            "Epoch 4, Iteration 1060\t train_loss: 3.3341 took: 1.2403s\n",
            "Epoch 4, Iteration 1080\t train_loss: 3.3357 took: 1.2451s\n",
            "Epoch 4, Iteration 1100\t train_loss: 3.3334 took: 1.1972s\n",
            "Epoch 4, Iteration 1120\t train_loss: 3.3337 took: 1.2214s\n",
            "Epoch 4, Iteration 1140\t train_loss: 3.3332 took: 1.2784s\n",
            "Epoch 4, Iteration 1160\t train_loss: 3.3346 took: 1.2140s\n",
            "Epoch 4, Iteration 1180\t train_loss: 3.3345 took: 1.1938s\n",
            "Epoch 4, Iteration 1200\t train_loss: 3.3315 took: 1.2472s\n",
            "index before test_loss: 44\n",
            "Validation loss = 2.2220\n",
            "Validation Accuracy = 0.5119\n",
            "Epoch 4, Iteration 1220\t train_loss: 3.3323 took: 1.4376s\n",
            "Epoch 4, Iteration 1240\t train_loss: 3.3340 took: 1.1905s\n",
            "Epoch 4, Iteration 1260\t train_loss: 3.3327 took: 1.2343s\n",
            "Epoch 4, Iteration 1280\t train_loss: 3.3325 took: 1.2164s\n",
            "Epoch 4, Iteration 1300\t train_loss: 3.3321 took: 1.2236s\n",
            "Epoch 4, Iteration 1320\t train_loss: 3.3330 took: 1.2513s\n",
            "Epoch 4, Iteration 1340\t train_loss: 3.3317 took: 1.2565s\n",
            "Epoch 4, Iteration 1360\t train_loss: 3.3341 took: 1.1792s\n",
            "Epoch 4, Iteration 1380\t train_loss: 3.3333 took: 1.2781s\n",
            "Epoch 4, Iteration 1400\t train_loss: 3.3355 took: 1.1465s\n",
            "index before test_loss: 45\n",
            "Validation loss = 2.2208\n",
            "Validation Accuracy = 0.4214\n",
            "Epoch 4, Iteration 1420\t train_loss: 3.3328 took: 1.4095s\n",
            "Epoch 4, Iteration 1440\t train_loss: 3.3316 took: 1.2595s\n",
            "Epoch 4, Iteration 1460\t train_loss: 3.3333 took: 1.1970s\n",
            "Epoch 4, Iteration 1480\t train_loss: 3.3321 took: 1.1707s\n",
            "Epoch 4, Iteration 1500\t train_loss: 3.3330 took: 1.2729s\n",
            "Epoch 4, Iteration 1520\t train_loss: 3.3313 took: 1.2074s\n",
            "Epoch 4, Iteration 1540\t train_loss: 3.3314 took: 1.2276s\n",
            "Epoch 4, Iteration 1560\t train_loss: 3.3355 took: 1.1893s\n",
            "Epoch 4, Iteration 1580\t train_loss: 3.3322 took: 1.2467s\n",
            "Epoch 4, Iteration 1600\t train_loss: 3.3314 took: 1.2402s\n",
            "index before test_loss: 46\n",
            "Validation loss = 2.2221\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 4, Iteration 1620\t train_loss: 3.3316 took: 1.3218s\n",
            "Epoch 4, Iteration 1640\t train_loss: 3.3330 took: 1.3014s\n",
            "Epoch 4, Iteration 1660\t train_loss: 3.3370 took: 1.2263s\n",
            "Epoch 4, Iteration 1680\t train_loss: 3.3336 took: 1.1972s\n",
            "Epoch 4, Iteration 1700\t train_loss: 3.3335 took: 1.2566s\n",
            "Epoch 4, Iteration 1720\t train_loss: 3.3342 took: 1.2416s\n",
            "Epoch 4, Iteration 1740\t train_loss: 3.3332 took: 1.2154s\n",
            "Epoch 4, Iteration 1760\t train_loss: 3.3339 took: 1.2062s\n",
            "Epoch 4, Iteration 1780\t train_loss: 3.3319 took: 1.2189s\n",
            "Epoch 4, Iteration 1800\t train_loss: 3.3336 took: 1.2837s\n",
            "index before test_loss: 47\n",
            "Validation loss = 2.2218\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 4, Iteration 1820\t train_loss: 3.3320 took: 1.4194s\n",
            "Epoch 4, Iteration 1840\t train_loss: 3.3333 took: 1.2132s\n",
            "Epoch 4, Iteration 1860\t train_loss: 3.3329 took: 1.2558s\n",
            "Epoch 4, Iteration 1880\t train_loss: 3.3305 took: 1.2151s\n",
            "Epoch 4, Iteration 1900\t train_loss: 3.3368 took: 1.2907s\n",
            "Epoch 4, Iteration 1920\t train_loss: 3.3328 took: 1.1940s\n",
            "Epoch 4, Iteration 1940\t train_loss: 3.3333 took: 1.2436s\n",
            "Epoch 4, Iteration 1960\t train_loss: 3.3350 took: 1.2229s\n",
            "Epoch 4, Iteration 1980\t train_loss: 3.3299 took: 1.2485s\n",
            "Epoch 4, Iteration 2000\t train_loss: 3.3339 took: 1.2228s\n",
            "index before test_loss: 48\n",
            "Validation loss = 2.2250\n",
            "Validation Accuracy = 0.4857\n",
            "Epoch 4, Iteration 2020\t train_loss: 3.3311 took: 1.3695s\n",
            "Epoch 4, Iteration 2040\t train_loss: 3.3333 took: 1.2248s\n",
            "Epoch 4, Iteration 2060\t train_loss: 3.3330 took: 1.1930s\n",
            "Epoch 4, Iteration 2080\t train_loss: 3.3350 took: 1.2117s\n",
            "Epoch 4, Iteration 2100\t train_loss: 3.3332 took: 1.2497s\n",
            "Epoch 4, Iteration 2120\t train_loss: 3.3327 took: 1.2407s\n",
            "Epoch 4, Iteration 2140\t train_loss: 3.3321 took: 1.2038s\n",
            "Epoch 4, Iteration 2160\t train_loss: 3.3314 took: 1.2481s\n",
            "Epoch 4, Iteration 2180\t train_loss: 3.3325 took: 1.2632s\n",
            "Epoch 4, Iteration 2200\t train_loss: 3.3337 took: 1.2070s\n",
            "index before test_loss: 49\n",
            "Validation loss = 2.2260\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 4, Iteration 2220\t train_loss: 3.3324 took: 1.3295s\n",
            "Epoch 4, Iteration 2240\t train_loss: 3.3352 took: 1.2080s\n",
            "Epoch 4, Iteration 2260\t train_loss: 3.3319 took: 1.2425s\n",
            "Epoch 4, Iteration 2280\t train_loss: 3.3335 took: 1.2038s\n",
            "Epoch 4, Iteration 2300\t train_loss: 3.3320 took: 1.3237s\n",
            "Epoch 4, Iteration 2320\t train_loss: 3.3331 took: 1.1804s\n",
            "Epoch 4, Iteration 2340\t train_loss: 3.3348 took: 1.1834s\n",
            "Epoch 4, Iteration 2360\t train_loss: 3.3323 took: 1.2325s\n",
            "Epoch 4, Iteration 2380\t train_loss: 3.3332 took: 1.2123s\n",
            "Epoch 4, Iteration 2400\t train_loss: 3.3325 took: 1.2154s\n",
            "index before test_loss: 50\n",
            "Validation loss = 2.2259\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 4, Iteration 2420\t train_loss: 3.3344 took: 1.3535s\n",
            "Epoch 4, Iteration 2440\t train_loss: 3.3327 took: 1.2289s\n",
            "Epoch 4, Iteration 2460\t train_loss: 3.3340 took: 1.2189s\n",
            "Epoch 4, Iteration 2480\t train_loss: 3.3327 took: 1.2106s\n",
            "Epoch 4, Iteration 2500\t train_loss: 3.3315 took: 1.2547s\n",
            "Epoch 4, Iteration 2520\t train_loss: 3.3332 took: 1.2456s\n",
            "Epoch 4, Iteration 2540\t train_loss: 3.3333 took: 1.2388s\n",
            "Epoch 4, Iteration 2560\t train_loss: 3.3315 took: 1.2058s\n",
            "Epoch 4, Iteration 2580\t train_loss: 3.3325 took: 1.2484s\n",
            "Epoch 4, Iteration 2600\t train_loss: 3.3326 took: 1.1998s\n",
            "index before test_loss: 51\n",
            "Validation loss = 2.2187\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 4, Iteration 2620\t train_loss: 3.3339 took: 1.3750s\n",
            "Epoch 5, Iteration 20\t train_loss: 3.3336 took: 1.3835s\n",
            "Epoch 5, Iteration 40\t train_loss: 3.3323 took: 1.2566s\n",
            "Epoch 5, Iteration 60\t train_loss: 3.3335 took: 1.2183s\n",
            "Epoch 5, Iteration 80\t train_loss: 3.3320 took: 1.1837s\n",
            "Epoch 5, Iteration 100\t train_loss: 3.3337 took: 1.2240s\n",
            "Epoch 5, Iteration 120\t train_loss: 3.3325 took: 1.1663s\n",
            "Epoch 5, Iteration 140\t train_loss: 3.3341 took: 1.2158s\n",
            "Epoch 5, Iteration 160\t train_loss: 3.3337 took: 1.2495s\n",
            "Epoch 5, Iteration 180\t train_loss: 3.3342 took: 1.1845s\n",
            "Epoch 5, Iteration 200\t train_loss: 3.3306 took: 1.1933s\n",
            "index before test_loss: 52\n",
            "Validation loss = 2.2198\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 5, Iteration 220\t train_loss: 3.3324 took: 1.4893s\n",
            "Epoch 5, Iteration 240\t train_loss: 3.3320 took: 1.2603s\n",
            "Epoch 5, Iteration 260\t train_loss: 3.3320 took: 1.2068s\n",
            "Epoch 5, Iteration 280\t train_loss: 3.3337 took: 1.2282s\n",
            "Epoch 5, Iteration 300\t train_loss: 3.3334 took: 1.2215s\n",
            "Epoch 5, Iteration 320\t train_loss: 3.3336 took: 1.2188s\n",
            "Epoch 5, Iteration 340\t train_loss: 3.3339 took: 1.1591s\n",
            "Epoch 5, Iteration 360\t train_loss: 3.3330 took: 1.2222s\n",
            "Epoch 5, Iteration 380\t train_loss: 3.3348 took: 1.2145s\n",
            "Epoch 5, Iteration 400\t train_loss: 3.3337 took: 1.2335s\n",
            "index before test_loss: 53\n",
            "Validation loss = 2.2244\n",
            "Validation Accuracy = 0.5286\n",
            "Epoch 5, Iteration 420\t train_loss: 3.3323 took: 1.3886s\n",
            "Epoch 5, Iteration 440\t train_loss: 3.3322 took: 1.1923s\n",
            "Epoch 5, Iteration 460\t train_loss: 3.3333 took: 1.2330s\n",
            "Epoch 5, Iteration 480\t train_loss: 3.3338 took: 1.2211s\n",
            "Epoch 5, Iteration 500\t train_loss: 3.3316 took: 1.2181s\n",
            "Epoch 5, Iteration 520\t train_loss: 3.3321 took: 1.2212s\n",
            "Epoch 5, Iteration 540\t train_loss: 3.3325 took: 1.1912s\n",
            "Epoch 5, Iteration 560\t train_loss: 3.3320 took: 1.2578s\n",
            "Epoch 5, Iteration 580\t train_loss: 3.3334 took: 1.2603s\n",
            "Epoch 5, Iteration 600\t train_loss: 3.3323 took: 1.1885s\n",
            "index before test_loss: 54\n",
            "Validation loss = 2.2268\n",
            "Validation Accuracy = 0.4405\n",
            "Epoch 5, Iteration 620\t train_loss: 3.3330 took: 1.3603s\n",
            "Epoch 5, Iteration 640\t train_loss: 3.3364 took: 1.2411s\n",
            "Epoch 5, Iteration 660\t train_loss: 3.3344 took: 1.1979s\n",
            "Epoch 5, Iteration 680\t train_loss: 3.3344 took: 1.1809s\n",
            "Epoch 5, Iteration 700\t train_loss: 3.3323 took: 1.2161s\n",
            "Epoch 5, Iteration 720\t train_loss: 3.3328 took: 1.2415s\n",
            "Epoch 5, Iteration 740\t train_loss: 3.3321 took: 1.2147s\n",
            "Epoch 5, Iteration 760\t train_loss: 3.3320 took: 1.1893s\n",
            "Epoch 5, Iteration 780\t train_loss: 3.3348 took: 1.1991s\n",
            "Epoch 5, Iteration 800\t train_loss: 3.3344 took: 1.1899s\n",
            "index before test_loss: 55\n",
            "Validation loss = 2.2214\n",
            "Validation Accuracy = 0.5548\n",
            "Epoch 5, Iteration 820\t train_loss: 3.3308 took: 1.4420s\n",
            "Epoch 5, Iteration 840\t train_loss: 3.3337 took: 1.1785s\n",
            "Epoch 5, Iteration 860\t train_loss: 3.3321 took: 1.2569s\n",
            "Epoch 5, Iteration 880\t train_loss: 3.3315 took: 1.2136s\n",
            "Epoch 5, Iteration 900\t train_loss: 3.3334 took: 1.1936s\n",
            "Epoch 5, Iteration 920\t train_loss: 3.3342 took: 1.2454s\n",
            "Epoch 5, Iteration 940\t train_loss: 3.3342 took: 1.1838s\n",
            "Epoch 5, Iteration 960\t train_loss: 3.3346 took: 1.2108s\n",
            "Epoch 5, Iteration 980\t train_loss: 3.3330 took: 1.2420s\n",
            "Epoch 5, Iteration 1000\t train_loss: 3.3318 took: 1.1869s\n",
            "index before test_loss: 56\n",
            "Validation loss = 2.2207\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 5, Iteration 1020\t train_loss: 3.3334 took: 1.3771s\n",
            "Epoch 5, Iteration 1040\t train_loss: 3.3347 took: 1.2616s\n",
            "Epoch 5, Iteration 1060\t train_loss: 3.3339 took: 1.2064s\n",
            "Epoch 5, Iteration 1080\t train_loss: 3.3330 took: 1.2328s\n",
            "Epoch 5, Iteration 1100\t train_loss: 3.3332 took: 1.1928s\n",
            "Epoch 5, Iteration 1120\t train_loss: 3.3342 took: 1.1818s\n",
            "Epoch 5, Iteration 1140\t train_loss: 3.3336 took: 1.2192s\n",
            "Epoch 5, Iteration 1160\t train_loss: 3.3331 took: 1.2103s\n",
            "Epoch 5, Iteration 1180\t train_loss: 3.3319 took: 1.2758s\n",
            "Epoch 5, Iteration 1200\t train_loss: 3.3321 took: 1.2998s\n",
            "index before test_loss: 57\n",
            "Validation loss = 2.2214\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 5, Iteration 1220\t train_loss: 3.3331 took: 1.3476s\n",
            "Epoch 5, Iteration 1240\t train_loss: 3.3319 took: 1.2287s\n",
            "Epoch 5, Iteration 1260\t train_loss: 3.3327 took: 1.1733s\n",
            "Epoch 5, Iteration 1280\t train_loss: 3.3315 took: 1.2618s\n",
            "Epoch 5, Iteration 1300\t train_loss: 3.3316 took: 1.2040s\n",
            "Epoch 5, Iteration 1320\t train_loss: 3.3325 took: 1.1667s\n",
            "Epoch 5, Iteration 1340\t train_loss: 3.3327 took: 1.2205s\n",
            "Epoch 5, Iteration 1360\t train_loss: 3.3354 took: 1.2307s\n",
            "Epoch 5, Iteration 1380\t train_loss: 3.3326 took: 1.2492s\n",
            "Epoch 5, Iteration 1400\t train_loss: 3.3321 took: 1.2435s\n",
            "index before test_loss: 58\n",
            "Validation loss = 2.2193\n",
            "Validation Accuracy = 0.4905\n",
            "Epoch 5, Iteration 1420\t train_loss: 3.3341 took: 1.3635s\n",
            "Epoch 5, Iteration 1440\t train_loss: 3.3351 took: 1.2734s\n",
            "Epoch 5, Iteration 1460\t train_loss: 3.3343 took: 1.1956s\n",
            "Epoch 5, Iteration 1480\t train_loss: 3.3325 took: 1.2377s\n",
            "Epoch 5, Iteration 1500\t train_loss: 3.3317 took: 1.1980s\n",
            "Epoch 5, Iteration 1520\t train_loss: 3.3343 took: 1.2059s\n",
            "Epoch 5, Iteration 1540\t train_loss: 3.3310 took: 1.2614s\n",
            "Epoch 5, Iteration 1560\t train_loss: 3.3352 took: 1.2579s\n",
            "Epoch 5, Iteration 1580\t train_loss: 3.3331 took: 1.1648s\n",
            "Epoch 5, Iteration 1600\t train_loss: 3.3338 took: 1.2144s\n",
            "index before test_loss: 59\n",
            "Validation loss = 2.2234\n",
            "Validation Accuracy = 0.4619\n",
            "Epoch 5, Iteration 1620\t train_loss: 3.3314 took: 1.3314s\n",
            "Epoch 5, Iteration 1640\t train_loss: 3.3313 took: 1.2220s\n",
            "Epoch 5, Iteration 1660\t train_loss: 3.3342 took: 1.2158s\n",
            "Epoch 5, Iteration 1680\t train_loss: 3.3332 took: 1.1868s\n",
            "Epoch 5, Iteration 1700\t train_loss: 3.3341 took: 1.2698s\n",
            "Epoch 5, Iteration 1720\t train_loss: 3.3339 took: 1.2255s\n",
            "Epoch 5, Iteration 1740\t train_loss: 3.3336 took: 1.2058s\n",
            "Epoch 5, Iteration 1760\t train_loss: 3.3338 took: 1.2265s\n",
            "Epoch 5, Iteration 1780\t train_loss: 3.3324 took: 1.2614s\n",
            "Epoch 5, Iteration 1800\t train_loss: 3.3317 took: 1.1667s\n",
            "index before test_loss: 60\n",
            "Validation loss = 2.2263\n",
            "Validation Accuracy = 0.4810\n",
            "Epoch 5, Iteration 1820\t train_loss: 3.3329 took: 1.3431s\n",
            "Epoch 5, Iteration 1840\t train_loss: 3.3321 took: 1.2396s\n",
            "Epoch 5, Iteration 1860\t train_loss: 3.3315 took: 1.2216s\n",
            "Epoch 5, Iteration 1880\t train_loss: 3.3334 took: 1.2569s\n",
            "Epoch 5, Iteration 1900\t train_loss: 3.3334 took: 1.2188s\n",
            "Epoch 5, Iteration 1920\t train_loss: 3.3325 took: 1.2246s\n",
            "Epoch 5, Iteration 1940\t train_loss: 3.3336 took: 1.1546s\n",
            "Epoch 5, Iteration 1960\t train_loss: 3.3321 took: 1.2375s\n",
            "Epoch 5, Iteration 1980\t train_loss: 3.3335 took: 1.2870s\n",
            "Epoch 5, Iteration 2000\t train_loss: 3.3324 took: 1.2065s\n",
            "index before test_loss: 61\n",
            "Validation loss = 2.2259\n",
            "Validation Accuracy = 0.4857\n",
            "Epoch 5, Iteration 2020\t train_loss: 3.3340 took: 1.4266s\n",
            "Epoch 5, Iteration 2040\t train_loss: 3.3311 took: 1.2170s\n",
            "Epoch 5, Iteration 2060\t train_loss: 3.3335 took: 1.2188s\n",
            "Epoch 5, Iteration 2080\t train_loss: 3.3344 took: 1.2716s\n",
            "Epoch 5, Iteration 2100\t train_loss: 3.3323 took: 1.2202s\n",
            "Epoch 5, Iteration 2120\t train_loss: 3.3326 took: 1.2779s\n",
            "Epoch 5, Iteration 2140\t train_loss: 3.3320 took: 1.1712s\n",
            "Epoch 5, Iteration 2160\t train_loss: 3.3333 took: 1.2181s\n",
            "Epoch 5, Iteration 2180\t train_loss: 3.3326 took: 1.1885s\n",
            "Epoch 5, Iteration 2200\t train_loss: 3.3340 took: 1.2475s\n",
            "index before test_loss: 62\n",
            "Validation loss = 2.2210\n",
            "Validation Accuracy = 0.4024\n",
            "Epoch 5, Iteration 2220\t train_loss: 3.3331 took: 1.3996s\n",
            "Epoch 5, Iteration 2240\t train_loss: 3.3353 took: 1.2179s\n",
            "Epoch 5, Iteration 2260\t train_loss: 3.3340 took: 1.2504s\n",
            "Epoch 5, Iteration 2280\t train_loss: 3.3353 took: 1.2419s\n",
            "Epoch 5, Iteration 2300\t train_loss: 3.3329 took: 1.1880s\n",
            "Epoch 5, Iteration 2320\t train_loss: 3.3320 took: 1.2518s\n",
            "Epoch 5, Iteration 2340\t train_loss: 3.3321 took: 1.2573s\n",
            "Epoch 5, Iteration 2360\t train_loss: 3.3324 took: 1.3023s\n",
            "Epoch 5, Iteration 2380\t train_loss: 3.3336 took: 1.1670s\n",
            "Epoch 5, Iteration 2400\t train_loss: 3.3313 took: 1.2053s\n",
            "index before test_loss: 63\n",
            "Validation loss = 2.2201\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 5, Iteration 2420\t train_loss: 3.3322 took: 1.3691s\n",
            "Epoch 5, Iteration 2440\t train_loss: 3.3296 took: 1.2386s\n",
            "Epoch 5, Iteration 2460\t train_loss: 3.3367 took: 1.2490s\n",
            "Epoch 5, Iteration 2480\t train_loss: 3.3327 took: 1.1740s\n",
            "Epoch 5, Iteration 2500\t train_loss: 3.3335 took: 1.1971s\n",
            "Epoch 5, Iteration 2520\t train_loss: 3.3326 took: 1.2438s\n",
            "Epoch 5, Iteration 2540\t train_loss: 3.3325 took: 1.3000s\n",
            "Epoch 5, Iteration 2560\t train_loss: 3.3335 took: 1.1473s\n",
            "Epoch 5, Iteration 2580\t train_loss: 3.3335 took: 1.2827s\n",
            "Epoch 5, Iteration 2600\t train_loss: 3.3326 took: 1.1964s\n",
            "index before test_loss: 64\n",
            "Validation loss = 2.2151\n",
            "Validation Accuracy = 0.4476\n",
            "Epoch 5, Iteration 2620\t train_loss: 3.3328 took: 1.4077s\n",
            "Epoch 6, Iteration 20\t train_loss: 3.3347 took: 1.3469s\n",
            "Epoch 6, Iteration 40\t train_loss: 3.3315 took: 1.2064s\n",
            "Epoch 6, Iteration 60\t train_loss: 3.3314 took: 1.2646s\n",
            "Epoch 6, Iteration 80\t train_loss: 3.3315 took: 1.2251s\n",
            "Epoch 6, Iteration 100\t train_loss: 3.3352 took: 1.2040s\n",
            "Epoch 6, Iteration 120\t train_loss: 3.3329 took: 1.2279s\n",
            "Epoch 6, Iteration 140\t train_loss: 3.3334 took: 1.2157s\n",
            "Epoch 6, Iteration 160\t train_loss: 3.3305 took: 1.3064s\n",
            "Epoch 6, Iteration 180\t train_loss: 3.3318 took: 1.1943s\n",
            "Epoch 6, Iteration 200\t train_loss: 3.3324 took: 1.2105s\n",
            "index before test_loss: 65\n",
            "Validation loss = 2.2191\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 6, Iteration 220\t train_loss: 3.3315 took: 1.4507s\n",
            "Epoch 6, Iteration 240\t train_loss: 3.3335 took: 1.2356s\n",
            "Epoch 6, Iteration 260\t train_loss: 3.3353 took: 1.2235s\n",
            "Epoch 6, Iteration 280\t train_loss: 3.3320 took: 1.2875s\n",
            "Epoch 6, Iteration 300\t train_loss: 3.3306 took: 1.2312s\n",
            "Epoch 6, Iteration 320\t train_loss: 3.3321 took: 1.2263s\n",
            "Epoch 6, Iteration 340\t train_loss: 3.3314 took: 1.1861s\n",
            "Epoch 6, Iteration 360\t train_loss: 3.3331 took: 1.2635s\n",
            "Epoch 6, Iteration 380\t train_loss: 3.3339 took: 1.1922s\n",
            "Epoch 6, Iteration 400\t train_loss: 3.3315 took: 1.2339s\n",
            "index before test_loss: 66\n",
            "Validation loss = 2.2195\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 6, Iteration 420\t train_loss: 3.3310 took: 1.3812s\n",
            "Epoch 6, Iteration 440\t train_loss: 3.3317 took: 1.2175s\n",
            "Epoch 6, Iteration 460\t train_loss: 3.3339 took: 1.2414s\n",
            "Epoch 6, Iteration 480\t train_loss: 3.3329 took: 1.1947s\n",
            "Epoch 6, Iteration 500\t train_loss: 3.3322 took: 1.2354s\n",
            "Epoch 6, Iteration 520\t train_loss: 3.3359 took: 1.3144s\n",
            "Epoch 6, Iteration 540\t train_loss: 3.3335 took: 1.1914s\n",
            "Epoch 6, Iteration 560\t train_loss: 3.3319 took: 1.2464s\n",
            "Epoch 6, Iteration 580\t train_loss: 3.3320 took: 1.2105s\n",
            "Epoch 6, Iteration 600\t train_loss: 3.3322 took: 1.2636s\n",
            "index before test_loss: 67\n",
            "Validation loss = 2.2194\n",
            "Validation Accuracy = 0.4476\n",
            "Epoch 6, Iteration 620\t train_loss: 3.3319 took: 1.3511s\n",
            "Epoch 6, Iteration 640\t train_loss: 3.3334 took: 1.2157s\n",
            "Epoch 6, Iteration 660\t train_loss: 3.3326 took: 1.1688s\n",
            "Epoch 6, Iteration 680\t train_loss: 3.3335 took: 1.2420s\n",
            "Epoch 6, Iteration 700\t train_loss: 3.3347 took: 1.2384s\n",
            "Epoch 6, Iteration 720\t train_loss: 3.3337 took: 1.2467s\n",
            "Epoch 6, Iteration 740\t train_loss: 3.3328 took: 1.2540s\n",
            "Epoch 6, Iteration 760\t train_loss: 3.3333 took: 1.2150s\n",
            "Epoch 6, Iteration 780\t train_loss: 3.3328 took: 1.1793s\n",
            "Epoch 6, Iteration 800\t train_loss: 3.3329 took: 1.1914s\n",
            "index before test_loss: 68\n",
            "Validation loss = 2.2204\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 6, Iteration 820\t train_loss: 3.3322 took: 1.3748s\n",
            "Epoch 6, Iteration 840\t train_loss: 3.3339 took: 1.2493s\n",
            "Epoch 6, Iteration 860\t train_loss: 3.3353 took: 1.2250s\n",
            "Epoch 6, Iteration 880\t train_loss: 3.3316 took: 1.2594s\n",
            "Epoch 6, Iteration 900\t train_loss: 3.3334 took: 1.1945s\n",
            "Epoch 6, Iteration 920\t train_loss: 3.3315 took: 1.2677s\n",
            "Epoch 6, Iteration 940\t train_loss: 3.3339 took: 1.2370s\n",
            "Epoch 6, Iteration 960\t train_loss: 3.3304 took: 1.2654s\n",
            "Epoch 6, Iteration 980\t train_loss: 3.3324 took: 1.1910s\n",
            "Epoch 6, Iteration 1000\t train_loss: 3.3342 took: 1.2514s\n",
            "index before test_loss: 69\n",
            "Validation loss = 2.2183\n",
            "Validation Accuracy = 0.4690\n",
            "Epoch 6, Iteration 1020\t train_loss: 3.3363 took: 1.3157s\n",
            "Epoch 6, Iteration 1040\t train_loss: 3.3328 took: 1.2349s\n",
            "Epoch 6, Iteration 1060\t train_loss: 3.3340 took: 1.2178s\n",
            "Epoch 6, Iteration 1080\t train_loss: 3.3314 took: 1.2770s\n",
            "Epoch 6, Iteration 1100\t train_loss: 3.3323 took: 1.2324s\n",
            "Epoch 6, Iteration 1120\t train_loss: 3.3343 took: 1.2558s\n",
            "Epoch 6, Iteration 1140\t train_loss: 3.3350 took: 1.2317s\n",
            "Epoch 6, Iteration 1160\t train_loss: 3.3332 took: 1.2514s\n",
            "Epoch 6, Iteration 1180\t train_loss: 3.3329 took: 1.1974s\n",
            "Epoch 6, Iteration 1200\t train_loss: 3.3332 took: 1.2277s\n",
            "index before test_loss: 70\n",
            "Validation loss = 2.2179\n",
            "Validation Accuracy = 0.4905\n",
            "Epoch 6, Iteration 1220\t train_loss: 3.3328 took: 1.3807s\n",
            "Epoch 6, Iteration 1240\t train_loss: 3.3343 took: 1.2780s\n",
            "Epoch 6, Iteration 1260\t train_loss: 3.3323 took: 1.1961s\n",
            "Epoch 6, Iteration 1280\t train_loss: 3.3324 took: 1.2156s\n",
            "Epoch 6, Iteration 1300\t train_loss: 3.3332 took: 1.2665s\n",
            "Epoch 6, Iteration 1320\t train_loss: 3.3314 took: 1.1972s\n",
            "Epoch 6, Iteration 1340\t train_loss: 3.3365 took: 1.2502s\n",
            "Epoch 6, Iteration 1360\t train_loss: 3.3321 took: 1.2557s\n",
            "Epoch 6, Iteration 1380\t train_loss: 3.3315 took: 1.2284s\n",
            "Epoch 6, Iteration 1400\t train_loss: 3.3349 took: 1.1963s\n",
            "index before test_loss: 71\n",
            "Validation loss = 2.2184\n",
            "Validation Accuracy = 0.4690\n",
            "Epoch 6, Iteration 1420\t train_loss: 3.3329 took: 1.4285s\n",
            "Epoch 6, Iteration 1440\t train_loss: 3.3321 took: 1.1786s\n",
            "Epoch 6, Iteration 1460\t train_loss: 3.3333 took: 1.2482s\n",
            "Epoch 6, Iteration 1480\t train_loss: 3.3333 took: 1.2193s\n",
            "Epoch 6, Iteration 1500\t train_loss: 3.3322 took: 1.2203s\n",
            "Epoch 6, Iteration 1520\t train_loss: 3.3329 took: 1.1770s\n",
            "Epoch 6, Iteration 1540\t train_loss: 3.3330 took: 1.2138s\n",
            "Epoch 6, Iteration 1560\t train_loss: 3.3340 took: 1.2629s\n",
            "Epoch 6, Iteration 1580\t train_loss: 3.3331 took: 1.2224s\n",
            "Epoch 6, Iteration 1600\t train_loss: 3.3316 took: 1.2461s\n",
            "index before test_loss: 72\n",
            "Validation loss = 2.2198\n",
            "Validation Accuracy = 0.4905\n",
            "Epoch 6, Iteration 1620\t train_loss: 3.3314 took: 1.3458s\n",
            "Epoch 6, Iteration 1640\t train_loss: 3.3343 took: 1.2256s\n",
            "Epoch 6, Iteration 1660\t train_loss: 3.3351 took: 1.2205s\n",
            "Epoch 6, Iteration 1680\t train_loss: 3.3315 took: 1.2146s\n",
            "Epoch 6, Iteration 1700\t train_loss: 3.3318 took: 1.2337s\n",
            "Epoch 6, Iteration 1720\t train_loss: 3.3318 took: 1.2705s\n",
            "Epoch 6, Iteration 1740\t train_loss: 3.3339 took: 1.2567s\n",
            "Epoch 6, Iteration 1760\t train_loss: 3.3313 took: 1.2080s\n",
            "Epoch 6, Iteration 1780\t train_loss: 3.3343 took: 1.2032s\n",
            "Epoch 6, Iteration 1800\t train_loss: 3.3375 took: 1.2767s\n",
            "index before test_loss: 73\n",
            "Validation loss = 2.2217\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 6, Iteration 1820\t train_loss: 3.3351 took: 1.3294s\n",
            "Epoch 6, Iteration 1840\t train_loss: 3.3311 took: 1.2233s\n",
            "Epoch 6, Iteration 1860\t train_loss: 3.3344 took: 1.2974s\n",
            "Epoch 6, Iteration 1880\t train_loss: 3.3337 took: 1.2267s\n",
            "Epoch 6, Iteration 1900\t train_loss: 3.3320 took: 1.2084s\n",
            "Epoch 6, Iteration 1920\t train_loss: 3.3327 took: 1.2307s\n",
            "Epoch 6, Iteration 1940\t train_loss: 3.3316 took: 1.2029s\n",
            "Epoch 6, Iteration 1960\t train_loss: 3.3330 took: 1.3011s\n",
            "Epoch 6, Iteration 1980\t train_loss: 3.3331 took: 1.1622s\n",
            "Epoch 6, Iteration 2000\t train_loss: 3.3308 took: 1.2465s\n",
            "index before test_loss: 74\n",
            "Validation loss = 2.2240\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 6, Iteration 2020\t train_loss: 3.3336 took: 1.3455s\n",
            "Epoch 6, Iteration 2040\t train_loss: 3.3294 took: 1.2275s\n",
            "Epoch 6, Iteration 2060\t train_loss: 3.3350 took: 1.2426s\n",
            "Epoch 6, Iteration 2080\t train_loss: 3.3336 took: 1.1748s\n",
            "Epoch 6, Iteration 2100\t train_loss: 3.3340 took: 1.2164s\n",
            "Epoch 6, Iteration 2120\t train_loss: 3.3317 took: 1.2458s\n",
            "Epoch 6, Iteration 2140\t train_loss: 3.3331 took: 1.2680s\n",
            "Epoch 6, Iteration 2160\t train_loss: 3.3328 took: 1.1559s\n",
            "Epoch 6, Iteration 2180\t train_loss: 3.3343 took: 1.2057s\n",
            "Epoch 6, Iteration 2200\t train_loss: 3.3327 took: 1.2477s\n",
            "index before test_loss: 75\n",
            "Validation loss = 2.2266\n",
            "Validation Accuracy = 0.3976\n",
            "Epoch 6, Iteration 2220\t train_loss: 3.3336 took: 1.3803s\n",
            "Epoch 6, Iteration 2240\t train_loss: 3.3328 took: 1.2068s\n",
            "Epoch 6, Iteration 2260\t train_loss: 3.3323 took: 1.1891s\n",
            "Epoch 6, Iteration 2280\t train_loss: 3.3334 took: 1.2394s\n",
            "Epoch 6, Iteration 2300\t train_loss: 3.3305 took: 1.2308s\n",
            "Epoch 6, Iteration 2320\t train_loss: 3.3335 took: 1.2576s\n",
            "Epoch 6, Iteration 2340\t train_loss: 3.3320 took: 1.1942s\n",
            "Epoch 6, Iteration 2360\t train_loss: 3.3340 took: 1.2489s\n",
            "Epoch 6, Iteration 2380\t train_loss: 3.3328 took: 1.2867s\n",
            "Epoch 6, Iteration 2400\t train_loss: 3.3349 took: 1.2164s\n",
            "index before test_loss: 76\n",
            "Validation loss = 2.2223\n",
            "Validation Accuracy = 0.4238\n",
            "Epoch 6, Iteration 2420\t train_loss: 3.3331 took: 1.3505s\n",
            "Epoch 6, Iteration 2440\t train_loss: 3.3310 took: 1.2413s\n",
            "Epoch 6, Iteration 2460\t train_loss: 3.3339 took: 1.2530s\n",
            "Epoch 6, Iteration 2480\t train_loss: 3.3348 took: 1.2203s\n",
            "Epoch 6, Iteration 2500\t train_loss: 3.3336 took: 1.2218s\n",
            "Epoch 6, Iteration 2520\t train_loss: 3.3325 took: 1.1889s\n",
            "Epoch 6, Iteration 2540\t train_loss: 3.3329 took: 1.2548s\n",
            "Epoch 6, Iteration 2560\t train_loss: 3.3314 took: 1.2231s\n",
            "Epoch 6, Iteration 2580\t train_loss: 3.3315 took: 1.2623s\n",
            "Epoch 6, Iteration 2600\t train_loss: 3.3350 took: 1.2630s\n",
            "index before test_loss: 77\n",
            "Validation loss = 2.2238\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 6, Iteration 2620\t train_loss: 3.3336 took: 1.3570s\n",
            "Epoch 7, Iteration 20\t train_loss: 3.3307 took: 1.3614s\n",
            "Epoch 7, Iteration 40\t train_loss: 3.3336 took: 1.2310s\n",
            "Epoch 7, Iteration 60\t train_loss: 3.3319 took: 1.2296s\n",
            "Epoch 7, Iteration 80\t train_loss: 3.3342 took: 1.2983s\n",
            "Epoch 7, Iteration 100\t train_loss: 3.3332 took: 1.2229s\n",
            "Epoch 7, Iteration 120\t train_loss: 3.3327 took: 1.2578s\n",
            "Epoch 7, Iteration 140\t train_loss: 3.3349 took: 1.1994s\n",
            "Epoch 7, Iteration 160\t train_loss: 3.3332 took: 1.2105s\n",
            "Epoch 7, Iteration 180\t train_loss: 3.3328 took: 1.2011s\n",
            "Epoch 7, Iteration 200\t train_loss: 3.3331 took: 1.2239s\n",
            "index before test_loss: 78\n",
            "Validation loss = 2.2256\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 7, Iteration 220\t train_loss: 3.3332 took: 1.3991s\n",
            "Epoch 7, Iteration 240\t train_loss: 3.3331 took: 1.2317s\n",
            "Epoch 7, Iteration 260\t train_loss: 3.3341 took: 1.2356s\n",
            "Epoch 7, Iteration 280\t train_loss: 3.3317 took: 1.1964s\n",
            "Epoch 7, Iteration 300\t train_loss: 3.3328 took: 1.2374s\n",
            "Epoch 7, Iteration 320\t train_loss: 3.3333 took: 1.2375s\n",
            "Epoch 7, Iteration 340\t train_loss: 3.3339 took: 1.1862s\n",
            "Epoch 7, Iteration 360\t train_loss: 3.3350 took: 1.2478s\n",
            "Epoch 7, Iteration 380\t train_loss: 3.3328 took: 1.1940s\n",
            "Epoch 7, Iteration 400\t train_loss: 3.3317 took: 1.2885s\n",
            "index before test_loss: 79\n",
            "Validation loss = 2.2225\n",
            "Validation Accuracy = 0.4190\n",
            "Epoch 7, Iteration 420\t train_loss: 3.3324 took: 1.3875s\n",
            "Epoch 7, Iteration 440\t train_loss: 3.3323 took: 1.2253s\n",
            "Epoch 7, Iteration 460\t train_loss: 3.3333 took: 1.2270s\n",
            "Epoch 7, Iteration 480\t train_loss: 3.3338 took: 1.1950s\n",
            "Epoch 7, Iteration 500\t train_loss: 3.3302 took: 1.2262s\n",
            "Epoch 7, Iteration 520\t train_loss: 3.3324 took: 1.2466s\n",
            "Epoch 7, Iteration 540\t train_loss: 3.3339 took: 1.2058s\n",
            "Epoch 7, Iteration 560\t train_loss: 3.3319 took: 1.2564s\n",
            "Epoch 7, Iteration 580\t train_loss: 3.3365 took: 1.2940s\n",
            "Epoch 7, Iteration 600\t train_loss: 3.3332 took: 1.2255s\n",
            "index before test_loss: 80\n",
            "Validation loss = 2.2228\n",
            "Validation Accuracy = 0.4429\n",
            "Epoch 7, Iteration 620\t train_loss: 3.3336 took: 1.3198s\n",
            "Epoch 7, Iteration 640\t train_loss: 3.3334 took: 1.2477s\n",
            "Epoch 7, Iteration 660\t train_loss: 3.3323 took: 1.1917s\n",
            "Epoch 7, Iteration 680\t train_loss: 3.3353 took: 1.2386s\n",
            "Epoch 7, Iteration 700\t train_loss: 3.3318 took: 1.2740s\n",
            "Epoch 7, Iteration 720\t train_loss: 3.3328 took: 1.2103s\n",
            "Epoch 7, Iteration 740\t train_loss: 3.3339 took: 1.2171s\n",
            "Epoch 7, Iteration 760\t train_loss: 3.3336 took: 1.2195s\n",
            "Epoch 7, Iteration 780\t train_loss: 3.3327 took: 1.2067s\n",
            "Epoch 7, Iteration 800\t train_loss: 3.3314 took: 1.2220s\n",
            "index before test_loss: 81\n",
            "Validation loss = 2.2256\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 7, Iteration 820\t train_loss: 3.3317 took: 1.3514s\n",
            "Epoch 7, Iteration 840\t train_loss: 3.3342 took: 1.2090s\n",
            "Epoch 7, Iteration 860\t train_loss: 3.3314 took: 1.2698s\n",
            "Epoch 7, Iteration 880\t train_loss: 3.3344 took: 1.2287s\n",
            "Epoch 7, Iteration 900\t train_loss: 3.3354 took: 1.1764s\n",
            "Epoch 7, Iteration 920\t train_loss: 3.3313 took: 1.2859s\n",
            "Epoch 7, Iteration 940\t train_loss: 3.3325 took: 1.2368s\n",
            "Epoch 7, Iteration 960\t train_loss: 3.3332 took: 1.1757s\n",
            "Epoch 7, Iteration 980\t train_loss: 3.3311 took: 1.2410s\n",
            "Epoch 7, Iteration 1000\t train_loss: 3.3334 took: 1.2514s\n",
            "index before test_loss: 82\n",
            "Validation loss = 2.2191\n",
            "Validation Accuracy = 0.5119\n",
            "Epoch 7, Iteration 1020\t train_loss: 3.3336 took: 1.3598s\n",
            "Epoch 7, Iteration 1040\t train_loss: 3.3351 took: 1.2203s\n",
            "Epoch 7, Iteration 1060\t train_loss: 3.3295 took: 1.2396s\n",
            "Epoch 7, Iteration 1080\t train_loss: 3.3325 took: 1.2834s\n",
            "Epoch 7, Iteration 1100\t train_loss: 3.3335 took: 1.2108s\n",
            "Epoch 7, Iteration 1120\t train_loss: 3.3329 took: 1.1829s\n",
            "Epoch 7, Iteration 1140\t train_loss: 3.3342 took: 1.2561s\n",
            "Epoch 7, Iteration 1160\t train_loss: 3.3337 took: 1.2144s\n",
            "Epoch 7, Iteration 1180\t train_loss: 3.3347 took: 1.2414s\n",
            "Epoch 7, Iteration 1200\t train_loss: 3.3328 took: 1.2368s\n",
            "index before test_loss: 83\n",
            "Validation loss = 2.2231\n",
            "Validation Accuracy = 0.4643\n",
            "Epoch 7, Iteration 1220\t train_loss: 3.3332 took: 1.3806s\n",
            "Epoch 7, Iteration 1240\t train_loss: 3.3325 took: 1.2220s\n",
            "Epoch 7, Iteration 1260\t train_loss: 3.3325 took: 1.2685s\n",
            "Epoch 7, Iteration 1280\t train_loss: 3.3326 took: 1.1990s\n",
            "Epoch 7, Iteration 1300\t train_loss: 3.3333 took: 1.2140s\n",
            "Epoch 7, Iteration 1320\t train_loss: 3.3352 took: 1.2285s\n",
            "Epoch 7, Iteration 1340\t train_loss: 3.3314 took: 1.2036s\n",
            "Epoch 7, Iteration 1360\t train_loss: 3.3322 took: 1.2870s\n",
            "Epoch 7, Iteration 1380\t train_loss: 3.3326 took: 1.2086s\n",
            "Epoch 7, Iteration 1400\t train_loss: 3.3329 took: 1.2856s\n",
            "index before test_loss: 84\n",
            "Validation loss = 2.2195\n",
            "Validation Accuracy = 0.4690\n",
            "Epoch 7, Iteration 1420\t train_loss: 3.3343 took: 1.3436s\n",
            "Epoch 7, Iteration 1440\t train_loss: 3.3312 took: 1.2353s\n",
            "Epoch 7, Iteration 1460\t train_loss: 3.3347 took: 1.2008s\n",
            "Epoch 7, Iteration 1480\t train_loss: 3.3341 took: 1.2220s\n",
            "Epoch 7, Iteration 1500\t train_loss: 3.3339 took: 1.2160s\n",
            "Epoch 7, Iteration 1520\t train_loss: 3.3333 took: 1.2400s\n",
            "Epoch 7, Iteration 1540\t train_loss: 3.3330 took: 1.2177s\n",
            "Epoch 7, Iteration 1560\t train_loss: 3.3326 took: 1.2369s\n",
            "Epoch 7, Iteration 1580\t train_loss: 3.3339 took: 1.2456s\n",
            "Epoch 7, Iteration 1600\t train_loss: 3.3337 took: 1.2833s\n",
            "index before test_loss: 85\n",
            "Validation loss = 2.2216\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 7, Iteration 1620\t train_loss: 3.3316 took: 1.3195s\n",
            "Epoch 7, Iteration 1640\t train_loss: 3.3332 took: 1.2624s\n",
            "Epoch 7, Iteration 1660\t train_loss: 3.3325 took: 1.2169s\n",
            "Epoch 7, Iteration 1680\t train_loss: 3.3337 took: 1.1920s\n",
            "Epoch 7, Iteration 1700\t train_loss: 3.3315 took: 1.2487s\n",
            "Epoch 7, Iteration 1720\t train_loss: 3.3346 took: 1.2404s\n",
            "Epoch 7, Iteration 1740\t train_loss: 3.3323 took: 1.2543s\n",
            "Epoch 7, Iteration 1760\t train_loss: 3.3328 took: 1.1877s\n",
            "Epoch 7, Iteration 1780\t train_loss: 3.3329 took: 1.2257s\n",
            "Epoch 7, Iteration 1800\t train_loss: 3.3334 took: 1.2421s\n",
            "index before test_loss: 86\n",
            "Validation loss = 2.2231\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 7, Iteration 1820\t train_loss: 3.3338 took: 1.3491s\n",
            "Epoch 7, Iteration 1840\t train_loss: 3.3339 took: 1.2721s\n",
            "Epoch 7, Iteration 1860\t train_loss: 3.3333 took: 1.2201s\n",
            "Epoch 7, Iteration 1880\t train_loss: 3.3316 took: 1.2001s\n",
            "Epoch 7, Iteration 1900\t train_loss: 3.3343 took: 1.2454s\n",
            "Epoch 7, Iteration 1920\t train_loss: 3.3336 took: 1.2485s\n",
            "Epoch 7, Iteration 1940\t train_loss: 3.3307 took: 1.2265s\n",
            "Epoch 7, Iteration 1960\t train_loss: 3.3339 took: 1.2200s\n",
            "Epoch 7, Iteration 1980\t train_loss: 3.3318 took: 1.2187s\n",
            "Epoch 7, Iteration 2000\t train_loss: 3.3315 took: 1.1918s\n",
            "index before test_loss: 87\n",
            "Validation loss = 2.2242\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 7, Iteration 2020\t train_loss: 3.3331 took: 1.4493s\n",
            "Epoch 7, Iteration 2040\t train_loss: 3.3339 took: 1.2770s\n",
            "Epoch 7, Iteration 2060\t train_loss: 3.3323 took: 1.1687s\n",
            "Epoch 7, Iteration 2080\t train_loss: 3.3346 took: 1.2638s\n",
            "Epoch 7, Iteration 2100\t train_loss: 3.3336 took: 1.1955s\n",
            "Epoch 7, Iteration 2120\t train_loss: 3.3328 took: 1.2520s\n",
            "Epoch 7, Iteration 2140\t train_loss: 3.3330 took: 1.2333s\n",
            "Epoch 7, Iteration 2160\t train_loss: 3.3321 took: 1.2308s\n",
            "Epoch 7, Iteration 2180\t train_loss: 3.3315 took: 1.1923s\n",
            "Epoch 7, Iteration 2200\t train_loss: 3.3351 took: 1.2567s\n",
            "index before test_loss: 88\n",
            "Validation loss = 2.2249\n",
            "Validation Accuracy = 0.4619\n",
            "Epoch 7, Iteration 2220\t train_loss: 3.3346 took: 1.3993s\n",
            "Epoch 7, Iteration 2240\t train_loss: 3.3329 took: 1.1996s\n",
            "Epoch 7, Iteration 2260\t train_loss: 3.3344 took: 1.2247s\n",
            "Epoch 7, Iteration 2280\t train_loss: 3.3328 took: 1.1992s\n",
            "Epoch 7, Iteration 2300\t train_loss: 3.3316 took: 1.1772s\n",
            "Epoch 7, Iteration 2320\t train_loss: 3.3314 took: 1.2727s\n",
            "Epoch 7, Iteration 2340\t train_loss: 3.3341 took: 1.1793s\n",
            "Epoch 7, Iteration 2360\t train_loss: 3.3345 took: 1.1990s\n",
            "Epoch 7, Iteration 2380\t train_loss: 3.3332 took: 1.2311s\n",
            "Epoch 7, Iteration 2400\t train_loss: 3.3329 took: 1.2926s\n",
            "index before test_loss: 89\n",
            "Validation loss = 2.2211\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 7, Iteration 2420\t train_loss: 3.3344 took: 1.3327s\n",
            "Epoch 7, Iteration 2440\t train_loss: 3.3319 took: 1.2123s\n",
            "Epoch 7, Iteration 2460\t train_loss: 3.3321 took: 1.2576s\n",
            "Epoch 7, Iteration 2480\t train_loss: 3.3315 took: 1.2184s\n",
            "Epoch 7, Iteration 2500\t train_loss: 3.3340 took: 1.2636s\n",
            "Epoch 7, Iteration 2520\t train_loss: 3.3313 took: 1.1938s\n",
            "Epoch 7, Iteration 2540\t train_loss: 3.3342 took: 1.2747s\n",
            "Epoch 7, Iteration 2560\t train_loss: 3.3335 took: 1.1880s\n",
            "Epoch 7, Iteration 2580\t train_loss: 3.3331 took: 1.1967s\n",
            "Epoch 7, Iteration 2600\t train_loss: 3.3330 took: 1.2507s\n",
            "index before test_loss: 90\n",
            "Validation loss = 2.2212\n",
            "Validation Accuracy = 0.4000\n",
            "Epoch 7, Iteration 2620\t train_loss: 3.3353 took: 1.3978s\n",
            "Epoch 8, Iteration 20\t train_loss: 3.3322 took: 1.3670s\n",
            "Epoch 8, Iteration 40\t train_loss: 3.3314 took: 1.2295s\n",
            "Epoch 8, Iteration 60\t train_loss: 3.3338 took: 1.2035s\n",
            "Epoch 8, Iteration 80\t train_loss: 3.3337 took: 1.2838s\n",
            "Epoch 8, Iteration 100\t train_loss: 3.3325 took: 1.1913s\n",
            "Epoch 8, Iteration 120\t train_loss: 3.3352 took: 1.2098s\n",
            "Epoch 8, Iteration 140\t train_loss: 3.3343 took: 1.1849s\n",
            "Epoch 8, Iteration 160\t train_loss: 3.3335 took: 1.2677s\n",
            "Epoch 8, Iteration 180\t train_loss: 3.3318 took: 1.2768s\n",
            "Epoch 8, Iteration 200\t train_loss: 3.3343 took: 1.2281s\n",
            "index before test_loss: 91\n",
            "Validation loss = 2.2189\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 8, Iteration 220\t train_loss: 3.3342 took: 1.3109s\n",
            "Epoch 8, Iteration 240\t train_loss: 3.3335 took: 1.2806s\n",
            "Epoch 8, Iteration 260\t train_loss: 3.3337 took: 1.2175s\n",
            "Epoch 8, Iteration 280\t train_loss: 3.3327 took: 1.2305s\n",
            "Epoch 8, Iteration 300\t train_loss: 3.3316 took: 1.2368s\n",
            "Epoch 8, Iteration 320\t train_loss: 3.3322 took: 1.2349s\n",
            "Epoch 8, Iteration 340\t train_loss: 3.3322 took: 1.2065s\n",
            "Epoch 8, Iteration 360\t train_loss: 3.3343 took: 1.2567s\n",
            "Epoch 8, Iteration 380\t train_loss: 3.3335 took: 1.2374s\n",
            "Epoch 8, Iteration 400\t train_loss: 3.3330 took: 1.2366s\n",
            "index before test_loss: 92\n",
            "Validation loss = 2.2224\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 8, Iteration 420\t train_loss: 3.3342 took: 1.3757s\n",
            "Epoch 8, Iteration 440\t train_loss: 3.3304 took: 1.2344s\n",
            "Epoch 8, Iteration 460\t train_loss: 3.3323 took: 1.2965s\n",
            "Epoch 8, Iteration 480\t train_loss: 3.3354 took: 1.1844s\n",
            "Epoch 8, Iteration 500\t train_loss: 3.3300 took: 1.1809s\n",
            "Epoch 8, Iteration 520\t train_loss: 3.3337 took: 1.2747s\n",
            "Epoch 8, Iteration 540\t train_loss: 3.3311 took: 1.2153s\n",
            "Epoch 8, Iteration 560\t train_loss: 3.3330 took: 1.2129s\n",
            "Epoch 8, Iteration 580\t train_loss: 3.3322 took: 1.2475s\n",
            "Epoch 8, Iteration 600\t train_loss: 3.3328 took: 1.1777s\n",
            "index before test_loss: 93\n",
            "Validation loss = 2.2250\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 8, Iteration 620\t train_loss: 3.3333 took: 1.3744s\n",
            "Epoch 8, Iteration 640\t train_loss: 3.3328 took: 1.2116s\n",
            "Epoch 8, Iteration 660\t train_loss: 3.3299 took: 1.2309s\n",
            "Epoch 8, Iteration 680\t train_loss: 3.3334 took: 1.2268s\n",
            "Epoch 8, Iteration 700\t train_loss: 3.3326 took: 1.2011s\n",
            "Epoch 8, Iteration 720\t train_loss: 3.3330 took: 1.2449s\n",
            "Epoch 8, Iteration 740\t train_loss: 3.3360 took: 1.2460s\n",
            "Epoch 8, Iteration 760\t train_loss: 3.3338 took: 1.2238s\n",
            "Epoch 8, Iteration 780\t train_loss: 3.3330 took: 1.1533s\n",
            "Epoch 8, Iteration 800\t train_loss: 3.3327 took: 1.2507s\n",
            "index before test_loss: 94\n",
            "Validation loss = 2.2249\n",
            "Validation Accuracy = 0.4857\n",
            "Epoch 8, Iteration 820\t train_loss: 3.3339 took: 1.3863s\n",
            "Epoch 8, Iteration 840\t train_loss: 3.3342 took: 1.2272s\n",
            "Epoch 8, Iteration 860\t train_loss: 3.3333 took: 1.2438s\n",
            "Epoch 8, Iteration 880\t train_loss: 3.3326 took: 1.1712s\n",
            "Epoch 8, Iteration 900\t train_loss: 3.3356 took: 1.3049s\n",
            "Epoch 8, Iteration 920\t train_loss: 3.3340 took: 1.2209s\n",
            "Epoch 8, Iteration 940\t train_loss: 3.3329 took: 1.1841s\n",
            "Epoch 8, Iteration 960\t train_loss: 3.3343 took: 1.2027s\n",
            "Epoch 8, Iteration 980\t train_loss: 3.3327 took: 1.1778s\n",
            "Epoch 8, Iteration 1000\t train_loss: 3.3318 took: 1.2380s\n",
            "index before test_loss: 95\n",
            "Validation loss = 2.2237\n",
            "Validation Accuracy = 0.5071\n",
            "Epoch 8, Iteration 1020\t train_loss: 3.3321 took: 1.3697s\n",
            "Epoch 8, Iteration 1040\t train_loss: 3.3322 took: 1.2678s\n",
            "Epoch 8, Iteration 1060\t train_loss: 3.3315 took: 1.2663s\n",
            "Epoch 8, Iteration 1080\t train_loss: 3.3310 took: 1.2288s\n",
            "Epoch 8, Iteration 1100\t train_loss: 3.3358 took: 1.1871s\n",
            "Epoch 8, Iteration 1120\t train_loss: 3.3337 took: 1.2141s\n",
            "Epoch 8, Iteration 1140\t train_loss: 3.3332 took: 1.2358s\n",
            "Epoch 8, Iteration 1160\t train_loss: 3.3336 took: 1.2137s\n",
            "Epoch 8, Iteration 1180\t train_loss: 3.3328 took: 1.2064s\n",
            "Epoch 8, Iteration 1200\t train_loss: 3.3322 took: 1.2159s\n",
            "index before test_loss: 96\n",
            "Validation loss = 2.2262\n",
            "Validation Accuracy = 0.5524\n",
            "Epoch 8, Iteration 1220\t train_loss: 3.3319 took: 1.4054s\n",
            "Epoch 8, Iteration 1240\t train_loss: 3.3305 took: 1.2195s\n",
            "Epoch 8, Iteration 1260\t train_loss: 3.3363 took: 1.1895s\n",
            "Epoch 8, Iteration 1280\t train_loss: 3.3350 took: 1.2619s\n",
            "Epoch 8, Iteration 1300\t train_loss: 3.3324 took: 1.1695s\n",
            "Epoch 8, Iteration 1320\t train_loss: 3.3324 took: 1.2570s\n",
            "Epoch 8, Iteration 1340\t train_loss: 3.3341 took: 1.1823s\n",
            "Epoch 8, Iteration 1360\t train_loss: 3.3324 took: 1.2175s\n",
            "Epoch 8, Iteration 1380\t train_loss: 3.3342 took: 1.1979s\n",
            "Epoch 8, Iteration 1400\t train_loss: 3.3318 took: 1.2656s\n",
            "index before test_loss: 97\n",
            "Validation loss = 2.2231\n",
            "Validation Accuracy = 0.4452\n",
            "Epoch 8, Iteration 1420\t train_loss: 3.3333 took: 1.3849s\n",
            "Epoch 8, Iteration 1440\t train_loss: 3.3324 took: 1.1911s\n",
            "Epoch 8, Iteration 1460\t train_loss: 3.3323 took: 1.2048s\n",
            "Epoch 8, Iteration 1480\t train_loss: 3.3341 took: 1.2537s\n",
            "Epoch 8, Iteration 1500\t train_loss: 3.3337 took: 1.2325s\n",
            "Epoch 8, Iteration 1520\t train_loss: 3.3325 took: 1.2053s\n",
            "Epoch 8, Iteration 1540\t train_loss: 3.3339 took: 1.2627s\n",
            "Epoch 8, Iteration 1560\t train_loss: 3.3342 took: 1.2213s\n",
            "Epoch 8, Iteration 1580\t train_loss: 3.3332 took: 1.2283s\n",
            "Epoch 8, Iteration 1600\t train_loss: 3.3330 took: 1.1984s\n",
            "index before test_loss: 98\n",
            "Validation loss = 2.2192\n",
            "Validation Accuracy = 0.4929\n",
            "Epoch 8, Iteration 1620\t train_loss: 3.3346 took: 1.4376s\n",
            "Epoch 8, Iteration 1640\t train_loss: 3.3319 took: 1.2296s\n",
            "Epoch 8, Iteration 1660\t train_loss: 3.3329 took: 1.1943s\n",
            "Epoch 8, Iteration 1680\t train_loss: 3.3323 took: 1.1958s\n",
            "Epoch 8, Iteration 1700\t train_loss: 3.3321 took: 1.2632s\n",
            "Epoch 8, Iteration 1720\t train_loss: 3.3333 took: 1.2925s\n",
            "Epoch 8, Iteration 1740\t train_loss: 3.3336 took: 1.2118s\n",
            "Epoch 8, Iteration 1760\t train_loss: 3.3324 took: 1.1968s\n",
            "Epoch 8, Iteration 1780\t train_loss: 3.3307 took: 1.2444s\n",
            "Epoch 8, Iteration 1800\t train_loss: 3.3326 took: 1.2344s\n",
            "index before test_loss: 99\n",
            "Validation loss = 2.2233\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 8, Iteration 1820\t train_loss: 3.3345 took: 1.4268s\n",
            "Epoch 8, Iteration 1840\t train_loss: 3.3313 took: 1.1404s\n",
            "Epoch 8, Iteration 1860\t train_loss: 3.3346 took: 1.2661s\n",
            "Epoch 8, Iteration 1880\t train_loss: 3.3354 took: 1.2178s\n",
            "Epoch 8, Iteration 1900\t train_loss: 3.3327 took: 1.2562s\n",
            "Epoch 8, Iteration 1920\t train_loss: 3.3337 took: 1.2458s\n",
            "Epoch 8, Iteration 1940\t train_loss: 3.3324 took: 1.1960s\n",
            "Epoch 8, Iteration 1960\t train_loss: 3.3334 took: 1.1965s\n",
            "Epoch 8, Iteration 1980\t train_loss: 3.3342 took: 1.2444s\n",
            "Epoch 8, Iteration 2000\t train_loss: 3.3318 took: 1.1625s\n",
            "index before test_loss: 100\n",
            "Validation loss = 2.2227\n",
            "Validation Accuracy = 0.4881\n",
            "Epoch 8, Iteration 2020\t train_loss: 3.3321 took: 1.4227s\n",
            "Epoch 8, Iteration 2040\t train_loss: 3.3319 took: 1.2198s\n",
            "Epoch 8, Iteration 2060\t train_loss: 3.3326 took: 1.2315s\n",
            "Epoch 8, Iteration 2080\t train_loss: 3.3332 took: 1.2002s\n",
            "Epoch 8, Iteration 2100\t train_loss: 3.3356 took: 1.2041s\n",
            "Epoch 8, Iteration 2120\t train_loss: 3.3306 took: 1.2411s\n",
            "Epoch 8, Iteration 2140\t train_loss: 3.3321 took: 1.2388s\n",
            "Epoch 8, Iteration 2160\t train_loss: 3.3319 took: 1.2017s\n",
            "Epoch 8, Iteration 2180\t train_loss: 3.3341 took: 1.2360s\n",
            "Epoch 8, Iteration 2200\t train_loss: 3.3325 took: 1.2306s\n",
            "index before test_loss: 101\n",
            "Validation loss = 2.2217\n",
            "Validation Accuracy = 0.4667\n",
            "Epoch 8, Iteration 2220\t train_loss: 3.3333 took: 1.4185s\n",
            "Epoch 8, Iteration 2240\t train_loss: 3.3346 took: 1.1932s\n",
            "Epoch 8, Iteration 2260\t train_loss: 3.3327 took: 1.2514s\n",
            "Epoch 8, Iteration 2280\t train_loss: 3.3336 took: 1.1965s\n",
            "Epoch 8, Iteration 2300\t train_loss: 3.3315 took: 1.3077s\n",
            "Epoch 8, Iteration 2320\t train_loss: 3.3325 took: 1.2265s\n",
            "Epoch 8, Iteration 2340\t train_loss: 3.3307 took: 1.1554s\n",
            "Epoch 8, Iteration 2360\t train_loss: 3.3346 took: 1.2394s\n",
            "Epoch 8, Iteration 2380\t train_loss: 3.3339 took: 1.1964s\n",
            "Epoch 8, Iteration 2400\t train_loss: 3.3335 took: 1.2347s\n",
            "index before test_loss: 102\n",
            "Validation loss = 2.2199\n",
            "Validation Accuracy = 0.4024\n",
            "Epoch 8, Iteration 2420\t train_loss: 3.3330 took: 1.3636s\n",
            "Epoch 8, Iteration 2440\t train_loss: 3.3339 took: 1.2026s\n",
            "Epoch 8, Iteration 2460\t train_loss: 3.3327 took: 1.2782s\n",
            "Epoch 8, Iteration 2480\t train_loss: 3.3337 took: 1.1624s\n",
            "Epoch 8, Iteration 2500\t train_loss: 3.3319 took: 1.2513s\n",
            "Epoch 8, Iteration 2520\t train_loss: 3.3343 took: 1.2502s\n",
            "Epoch 8, Iteration 2540\t train_loss: 3.3344 took: 1.2516s\n",
            "Epoch 8, Iteration 2560\t train_loss: 3.3326 took: 1.2144s\n",
            "Epoch 8, Iteration 2580\t train_loss: 3.3346 took: 1.2184s\n",
            "Epoch 8, Iteration 2600\t train_loss: 3.3331 took: 1.2314s\n",
            "index before test_loss: 103\n",
            "Validation loss = 2.2186\n",
            "Validation Accuracy = 0.5357\n",
            "Epoch 8, Iteration 2620\t train_loss: 3.3337 took: 1.4067s\n",
            "Training finished, took 1308.90s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42MYWLufSbiu",
        "colab_type": "code",
        "outputId": "063832f9-5d8f-48a2-a87c-c3e8a787d6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4EoI12CyAYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e5b4ab65-ca4c-443b-ec04-388c7cef67f6"
      },
      "source": [
        "print(train_hist_x)\n",
        "print(train_loss_hist)\n",
        "print(test_hist_x)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoCtvRZjop9q",
        "colab_type": "code",
        "outputId": "a4cf96c6-29cc-4905-c722-ad4ffa7c42f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# loading and saving via this link:\n",
        "# https://medium.com/udacity-pytorch-challengers/saving-loading-your-model-in-pytorch-741b80daf3c\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "checkpoint = {'model': myCNN(),\n",
        "        'state_dict': net.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict()\n",
        "        }\n",
        "f = open('/content/gdrive/My Drive/asl_pretrained.pth', 'wb')\n",
        "torch.save(checkpoint, f)\n",
        "f.close()\n",
        "from google.colab import files\n",
        "files.download('/content/gdrive/My Drive/asl_pretrained.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type myCNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYS5ybbHPdyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLm7jzsDvRaZ",
        "colab_type": "code",
        "outputId": "9405afd7-a0d2-49d5-a481-fdb6470dd248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = myCNN()\n",
        "\n",
        "if use_pretrained_model:\n",
        "    gdown.download('https://drive.google.com/uc?authuser=0&id=1SDCCXQ67aM9QnMT5_419_x5Ek6_ueQKT&export=download',\n",
        "                   'asl_pretrained(5).pth',\n",
        "                   quiet=False)\n",
        "    #https://drive.google.com/open?id=1SDCCXQ67aM9QnMT5_419_x5Ek6_ueQKT\n",
        "    #https://drive.google.com/open?id=1w4RW2U8QMK3kw8A8oA-DEhglInU4uy0a\n",
        "    check_point = torch.load('asl_pretrained(5).pth')\n",
        "    \n",
        "    net.load_state_dict(check_point['state_dict'])\n",
        "\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "\n",
        "# Visualize the architecture of the model\n",
        "# We need to give the net a fake input for this library to visualize the architecture\n",
        "fake_input = Variable(torch.zeros((1,image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "outputs = net(fake_input)\n",
        "# Plot the DAG (Directed Acyclic Graph) of the model\n",
        "make_dot(outputs, dict(net.named_parameters()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=1SDCCXQ67aM9QnMT5_419_x5Ek6_ueQKT&export=download\n",
            "To: /content/asl_pretrained(5).pth\n",
            "15.8MB [00:00, 82.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f0689d302e8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"468pt\" height=\"864pt\"\n viewBox=\"0.00 0.00 468.38 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.8268 .8268) rotate(0) translate(4 1041)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1041 562.5,-1041 562.5,4 -4,4\"/>\n<!-- 139666058122576 -->\n<g id=\"node1\" class=\"node\">\n<title>139666058122576</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"471,-21 367,-21 367,0 471,0 471,-21\"/>\n<text text-anchor=\"middle\" x=\"419\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139666058123248 -->\n<g id=\"node2\" class=\"node\">\n<title>139666058123248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"354,-91 300,-91 300,-57 354,-57 354,-91\"/>\n<text text-anchor=\"middle\" x=\"327\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"327\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (28)</text>\n</g>\n<!-- 139666058123248&#45;&gt;139666058122576 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139666058123248&#45;&gt;139666058122576</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M351.6543,-56.9832C365.1894,-47.641 381.8926,-36.1122 395.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"397.2865,-29.7398 403.5283,-21.1788 393.3102,-23.9788 397.2865,-29.7398\"/>\n</g>\n<!-- 139666058121568 -->\n<g id=\"node3\" class=\"node\">\n<title>139666058121568</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"466,-84.5 372,-84.5 372,-63.5 466,-63.5 466,-84.5\"/>\n<text text-anchor=\"middle\" x=\"419\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139666058121568&#45;&gt;139666058122576 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139666058121568&#45;&gt;139666058122576</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M419,-63.2281C419,-54.5091 419,-41.9699 419,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"422.5001,-31.1128 419,-21.1128 415.5001,-31.1129 422.5001,-31.1128\"/>\n</g>\n<!-- 139666058122408 -->\n<g id=\"node4\" class=\"node\">\n<title>139666058122408</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"470,-154.5 366,-154.5 366,-133.5 470,-133.5 470,-154.5\"/>\n<text text-anchor=\"middle\" x=\"418\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139666058122408&#45;&gt;139666058121568 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139666058122408&#45;&gt;139666058121568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M418.1519,-133.3685C418.2972,-123.1925 418.5206,-107.5606 418.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"422.2034,-94.7806 418.8467,-84.7315 415.2041,-94.6805 422.2034,-94.7806\"/>\n</g>\n<!-- 139666058123584 -->\n<g id=\"node5\" class=\"node\">\n<title>139666058123584</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"354,-231 300,-231 300,-197 354,-197 354,-231\"/>\n<text text-anchor=\"middle\" x=\"327\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"327\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139666058123584&#45;&gt;139666058122408 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139666058123584&#45;&gt;139666058122408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M349.4944,-196.6966C363.7034,-185.7666 381.9745,-171.7119 396.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"398.447,-163.4565 404.2393,-154.5852 394.179,-157.9081 398.447,-163.4565\"/>\n</g>\n<!-- 139666058122856 -->\n<g id=\"node6\" class=\"node\">\n<title>139666058122856</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"463.5,-224.5 372.5,-224.5 372.5,-203.5 463.5,-203.5 463.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"418\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139666058122856&#45;&gt;139666058122408 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139666058122856&#45;&gt;139666058122408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M418,-203.3685C418,-193.1925 418,-177.5606 418,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.5001,-164.7315 418,-154.7315 414.5001,-164.7316 421.5001,-164.7315\"/>\n</g>\n<!-- 139666058123024 -->\n<g id=\"node7\" class=\"node\">\n<title>139666058123024</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"464,-294.5 370,-294.5 370,-273.5 464,-273.5 464,-294.5\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139666058123024&#45;&gt;139666058122856 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139666058123024&#45;&gt;139666058122856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417.1519,-273.3685C417.2972,-263.1925 417.5206,-247.5606 417.7016,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.2034,-234.7806 417.8467,-224.7315 414.2041,-234.6805 421.2034,-234.7806\"/>\n</g>\n<!-- 139666058124536 -->\n<g id=\"node8\" class=\"node\">\n<title>139666058124536</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"507,-358 327,-358 327,-337 507,-337 507,-358\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139666058124536&#45;&gt;139666058123024 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139666058124536&#45;&gt;139666058123024</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-336.7281C417,-328.0091 417,-315.4699 417,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-304.6128 417,-294.6128 413.5001,-304.6129 420.5001,-304.6128\"/>\n</g>\n<!-- 139666058122184 -->\n<g id=\"node9\" class=\"node\">\n<title>139666058122184</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"495.5,-415 338.5,-415 338.5,-394 495.5,-394 495.5,-415\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139666058122184&#45;&gt;139666058124536 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139666058122184&#45;&gt;139666058124536</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-393.7787C417,-386.6134 417,-376.9517 417,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-368.1732 417,-358.1732 413.5001,-368.1732 420.5001,-368.1732\"/>\n</g>\n<!-- 139666058123864 -->\n<g id=\"node10\" class=\"node\">\n<title>139666058123864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"352,-478.5 258,-478.5 258,-457.5 352,-457.5 352,-478.5\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-464.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139666058123864&#45;&gt;139666058122184 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139666058123864&#45;&gt;139666058122184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.7463,-457.3715C341.675,-447.2066 368.9273,-431.7555 389.3486,-420.1774\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.2197,-423.14 398.1926,-415.1631 387.7672,-417.0506 391.2197,-423.14\"/>\n</g>\n<!-- 139666058125152 -->\n<g id=\"node11\" class=\"node\">\n<title>139666058125152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"395,-542 215,-542 215,-521 395,-521 395,-542\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-528.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139666058125152&#45;&gt;139666058123864 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139666058125152&#45;&gt;139666058123864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-520.7281C305,-512.0091 305,-499.4699 305,-488.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-488.6128 305,-478.6128 301.5001,-488.6129 308.5001,-488.6128\"/>\n</g>\n<!-- 139666058123976 -->\n<g id=\"node12\" class=\"node\">\n<title>139666058123976</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"383.5,-599 226.5,-599 226.5,-578 383.5,-578 383.5,-599\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-585.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139666058123976&#45;&gt;139666058125152 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139666058123976&#45;&gt;139666058125152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-577.7787C305,-570.6134 305,-560.9517 305,-552.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-552.1732 305,-542.1732 301.5001,-552.1732 308.5001,-552.1732\"/>\n</g>\n<!-- 139666058124984 -->\n<g id=\"node13\" class=\"node\">\n<title>139666058124984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"243,-662.5 149,-662.5 149,-641.5 243,-641.5 243,-662.5\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-648.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139666058124984&#45;&gt;139666058123976 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139666058124984&#45;&gt;139666058123976</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.4904,-641.2281C231.8986,-631.0866 258.1791,-615.7764 277.9437,-604.2622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"279.9039,-607.1709 286.7827,-599.1128 276.3802,-601.1224 279.9039,-607.1709\"/>\n</g>\n<!-- 139666058123752 -->\n<g id=\"node14\" class=\"node\">\n<title>139666058123752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"286,-726 106,-726 106,-705 286,-705 286,-726\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-712.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139666058123752&#45;&gt;139666058124984 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139666058123752&#45;&gt;139666058124984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-704.7281C196,-696.0091 196,-683.4699 196,-672.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-672.6128 196,-662.6128 192.5001,-672.6129 199.5001,-672.6128\"/>\n</g>\n<!-- 139666058124592 -->\n<g id=\"node15\" class=\"node\">\n<title>139666058124592</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"274.5,-783 117.5,-783 117.5,-762 274.5,-762 274.5,-783\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-769.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139666058124592&#45;&gt;139666058123752 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139666058124592&#45;&gt;139666058123752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-761.7787C196,-754.6134 196,-744.9517 196,-736.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-736.1732 196,-726.1732 192.5001,-736.1732 199.5001,-736.1732\"/>\n</g>\n<!-- 139666058125208 -->\n<g id=\"node16\" class=\"node\">\n<title>139666058125208</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"137,-846.5 43,-846.5 43,-825.5 137,-825.5 137,-846.5\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-832.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139666058125208&#45;&gt;139666058124592 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139666058125208&#45;&gt;139666058124592</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M107.9815,-825.2281C124.9106,-815.0866 150.4677,-799.7764 169.6884,-788.2622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"171.5043,-791.2544 178.2841,-783.1128 167.9069,-785.2494 171.5043,-791.2544\"/>\n</g>\n<!-- 139666058122968 -->\n<g id=\"node17\" class=\"node\">\n<title>139666058122968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"180,-910 0,-910 0,-889 180,-889 180,-910\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-896.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139666058122968&#45;&gt;139666058125208 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139666058122968&#45;&gt;139666058125208</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90,-888.7281C90,-880.0091 90,-867.4699 90,-856.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.5001,-856.6128 90,-846.6128 86.5001,-856.6129 93.5001,-856.6128\"/>\n</g>\n<!-- 139666058123528 -->\n<g id=\"node18\" class=\"node\">\n<title>139666058123528</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"168.5,-967 11.5,-967 11.5,-946 168.5,-946 168.5,-967\"/>\n<text text-anchor=\"middle\" x=\"90\" y=\"-953.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139666058123528&#45;&gt;139666058122968 -->\n<g id=\"edge17\" class=\"edge\">\n<title>139666058123528&#45;&gt;139666058122968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90,-945.7787C90,-938.6134 90,-928.9517 90,-920.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.5001,-920.1732 90,-910.1732 86.5001,-920.1732 93.5001,-920.1732\"/>\n</g>\n<!-- 139666058123360 -->\n<g id=\"node19\" class=\"node\">\n<title>139666058123360</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"84.5,-1037 3.5,-1037 3.5,-1003 84.5,-1003 84.5,-1037\"/>\n<text text-anchor=\"middle\" x=\"44\" y=\"-1023.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"44\" y=\"-1010.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32, 3, 3, 3)</text>\n</g>\n<!-- 139666058123360&#45;&gt;139666058123528 -->\n<g id=\"edge18\" class=\"edge\">\n<title>139666058123360&#45;&gt;139666058123528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M56.3272,-1002.9832C62.4107,-994.5853 69.7742,-984.4204 76.0621,-975.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"79.0945,-977.5204 82.1266,-967.3687 73.4256,-973.4138 79.0945,-977.5204\"/>\n</g>\n<!-- 139666058217008 -->\n<g id=\"node20\" class=\"node\">\n<title>139666058217008</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"171,-1037 103,-1037 103,-1003 171,-1003 171,-1037\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-1023.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"137\" y=\"-1010.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (32)</text>\n</g>\n<!-- 139666058217008&#45;&gt;139666058123528 -->\n<g id=\"edge19\" class=\"edge\">\n<title>139666058217008&#45;&gt;139666058123528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M124.4049,-1002.9832C118.1237,-994.4969 110.5069,-984.2062 104.0384,-975.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.8071,-973.3243 98.0445,-967.3687 101.1806,-977.4888 106.8071,-973.3243\"/>\n</g>\n<!-- 139666058122464 -->\n<g id=\"node21\" class=\"node\">\n<title>139666058122464</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"236.5,-853 155.5,-853 155.5,-819 236.5,-819 236.5,-853\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-839.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n<text text-anchor=\"middle\" x=\"196\" y=\"-826.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 32, 3, 3)</text>\n</g>\n<!-- 139666058122464&#45;&gt;139666058124592 -->\n<g id=\"edge20\" class=\"edge\">\n<title>139666058122464&#45;&gt;139666058124592</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M196,-818.9832C196,-811.1157 196,-801.6973 196,-793.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.5001,-793.3686 196,-783.3687 192.5001,-793.3687 199.5001,-793.3686\"/>\n</g>\n<!-- 139666058123696 -->\n<g id=\"node22\" class=\"node\">\n<title>139666058123696</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"323,-853 255,-853 255,-819 323,-819 323,-853\"/>\n<text text-anchor=\"middle\" x=\"289\" y=\"-839.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n<text text-anchor=\"middle\" x=\"289\" y=\"-826.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139666058123696&#45;&gt;139666058124592 -->\n<g id=\"edge21\" class=\"edge\">\n<title>139666058123696&#45;&gt;139666058124592</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M264.0777,-818.9832C250.3955,-809.641 233.5107,-798.1122 219.9799,-788.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"221.872,-785.9273 211.6398,-783.1788 217.9248,-791.7082 221.872,-785.9273\"/>\n</g>\n<!-- 139666058124256 -->\n<g id=\"node23\" class=\"node\">\n<title>139666058124256</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"348.5,-669 261.5,-669 261.5,-635 348.5,-635 348.5,-669\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv3.weight</text>\n<text text-anchor=\"middle\" x=\"305\" y=\"-642.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 64, 3, 3)</text>\n</g>\n<!-- 139666058124256&#45;&gt;139666058123976 -->\n<g id=\"edge22\" class=\"edge\">\n<title>139666058124256&#45;&gt;139666058123976</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M305,-634.9832C305,-627.1157 305,-617.6973 305,-609.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.5001,-609.3686 305,-599.3687 301.5001,-609.3687 308.5001,-609.3686\"/>\n</g>\n<!-- 139666058122016 -->\n<g id=\"node24\" class=\"node\">\n<title>139666058122016</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"435,-669 367,-669 367,-635 435,-635 435,-669\"/>\n<text text-anchor=\"middle\" x=\"401\" y=\"-655.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv3.bias</text>\n<text text-anchor=\"middle\" x=\"401\" y=\"-642.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 139666058122016&#45;&gt;139666058123976 -->\n<g id=\"edge23\" class=\"edge\">\n<title>139666058122016&#45;&gt;139666058123976</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M375.2738,-634.9832C361.1502,-625.641 343.7208,-614.1122 329.7534,-604.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"331.4158,-601.7766 321.1444,-599.1788 327.5539,-607.6149 331.4158,-601.7766\"/>\n</g>\n<!-- 139666058121456 -->\n<g id=\"node25\" class=\"node\">\n<title>139666058121456</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"463.5,-485 370.5,-485 370.5,-451 463.5,-451 463.5,-485\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv4.weight</text>\n<text text-anchor=\"middle\" x=\"417\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 128, 3, 3)</text>\n</g>\n<!-- 139666058121456&#45;&gt;139666058122184 -->\n<g id=\"edge24\" class=\"edge\">\n<title>139666058121456&#45;&gt;139666058122184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417,-450.9832C417,-443.1157 417,-433.6973 417,-425.4019\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.5001,-425.3686 417,-415.3687 413.5001,-425.3687 420.5001,-425.3686\"/>\n</g>\n<!-- 139666058121288 -->\n<g id=\"node26\" class=\"node\">\n<title>139666058121288</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"550,-485 482,-485 482,-451 550,-451 550,-485\"/>\n<text text-anchor=\"middle\" x=\"516\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv4.bias</text>\n<text text-anchor=\"middle\" x=\"516\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139666058121288&#45;&gt;139666058122184 -->\n<g id=\"edge25\" class=\"edge\">\n<title>139666058121288&#45;&gt;139666058122184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M489.4698,-450.9832C474.7662,-441.552 456.5881,-429.8924 442.1164,-420.61\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.9558,-417.6318 433.6489,-415.1788 440.1765,-423.5239 443.9558,-417.6318\"/>\n</g>\n<!-- 139666058123304 -->\n<g id=\"node27\" class=\"node\">\n<title>139666058123304</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"555.5,-224.5 482.5,-224.5 482.5,-203.5 555.5,-203.5 555.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"519\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139666058123304&#45;&gt;139666058122408 -->\n<g id=\"edge26\" class=\"edge\">\n<title>139666058123304&#45;&gt;139666058122408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M503.6603,-203.3685C487.1024,-191.8927 460.533,-173.4783 441.3726,-160.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.3659,-157.3219 433.1532,-154.5022 439.3784,-163.0752 443.3659,-157.3219\"/>\n</g>\n<!-- 139666058123472 -->\n<g id=\"node28\" class=\"node\">\n<title>139666058123472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"557.5,-301 482.5,-301 482.5,-267 557.5,-267 557.5,-301\"/>\n<text text-anchor=\"middle\" x=\"520\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"520\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 2304)</text>\n</g>\n<!-- 139666058123472&#45;&gt;139666058123304 -->\n<g id=\"edge27\" class=\"edge\">\n<title>139666058123472&#45;&gt;139666058123304</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M519.7528,-266.6966C519.6152,-257.0634 519.4429,-245.003 519.2979,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"522.7967,-234.7402 519.1542,-224.7913 515.7975,-234.8403 522.7967,-234.7402\"/>\n</g>\n<!-- 139666058123080 -->\n<g id=\"node29\" class=\"node\">\n<title>139666058123080</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"558.5,-84.5 485.5,-84.5 485.5,-63.5 558.5,-63.5 558.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"522\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139666058123080&#45;&gt;139666058122576 -->\n<g id=\"edge28\" class=\"edge\">\n<title>139666058123080&#45;&gt;139666058122576</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M504.5275,-63.2281C488.1519,-53.1325 463.4682,-37.9149 444.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.5636,-23.3814 436.2145,-21.1128 442.8901,-29.3401 446.5636,-23.3814\"/>\n</g>\n<!-- 139666058123192 -->\n<g id=\"node30\" class=\"node\">\n<title>139666058123192</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"555.5,-161 488.5,-161 488.5,-127 555.5,-127 555.5,-161\"/>\n<text text-anchor=\"middle\" x=\"522\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"522\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (28, 256)</text>\n</g>\n<!-- 139666058123192&#45;&gt;139666058123080 -->\n<g id=\"edge29\" class=\"edge\">\n<title>139666058123192&#45;&gt;139666058123080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M522,-126.6966C522,-117.0634 522,-105.003 522,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"525.5001,-94.7912 522,-84.7913 518.5001,-94.7913 525.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B049HQA_Pd05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuhdfQnUPd3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO9_2RKMPd5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSyd4qwzR2IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### sadness ####\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision.datasets import VisionDataset\n",
        "import os\n",
        "from glob import glob\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(img_tensor.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Datasets must always subclass either Dataset (either directly or indirectly)\n",
        "# Here, we use subclass the VisionDataset class, which is more standard for\n",
        "# computer vision datasets.\n",
        "class Caltech256(VisionDataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "        # make sure to call the super class init method\n",
        "        super(Caltech256, self).__init__('.',\n",
        "                                         transform=transform,\n",
        "                                         target_transform=target_transform)\n",
        "        \n",
        "        # we'll keep track of the categories here\n",
        "        self.categories = []\n",
        "        # the index will help us find the jpegs to load\n",
        "        self.index = []\n",
        "        # the y list will be used to determine the object category\n",
        "        self.y = []\n",
        "        # all of the data is extracted to the 256_ObjectCategories directory\n",
        "        # we search for all files that match ???.* (three characters followed\n",
        "        # by a . followed by any string).  This pattern matches all of the\n",
        "        # object directories we are interested in parsing.\n",
        "\n",
        "        class_mappings = {'A': 0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25,'space':26,'del':27,'nothing':28}\n",
        "\n",
        "        for c in sorted(glob(os.path.join(self.root, \"asl_alphabet_train\",\"asl_alphabet_train\",\"*\"))):\n",
        "            #print(c)\n",
        "            # get just the object category directory\n",
        "            _, category_dir = os.path.split(c)\n",
        "            # convert from 1 index to 0 index class\n",
        "            class_idx = class_mappings[category_dir]\n",
        "            #print(class_idx)\n",
        "            # there is an extra background class that we don't care about\n",
        "            # count the jpegs in the appropriate directory\n",
        "            img_files = glob(os.path.join(self.root, \"asl_alphabet_train\",\"asl_alphabet_train\", category_dir, '*.jpg'))\n",
        "            n = len(img_files)\n",
        "            # populate the categories\n",
        "            #self.categories.append(category_dir)\n",
        "            self.index.extend(img_files)\n",
        "            self.y.extend(n * [class_idx])\n",
        "            #print(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        # load the image using PIL\n",
        "        # a gotcha is when some of the images are black and white, we can use\n",
        "        # the convert('RGB') command to make sure everything is a three channel\n",
        "        # RGB image.\n",
        "        img = Image.open(self.index[index]).convert('RGB')\n",
        "        # the target has been cached in y\n",
        "        target = self.y[index]\n",
        "\n",
        "        # apply transformations if they exist (this is useful for images)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # apply transformations if they exist (this is useful for images)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        # you need to say how much data you have\n",
        "        return len(self.index)\n",
        "\n",
        "\n",
        "# center crop 200, 200 pixel patch and then resize to 100 by 100 for\n",
        "# computational efficiency\n",
        "cal_tech = Caltech256(transform=transforms.Compose([transforms.CenterCrop((200,200)),\n",
        "                                                    transforms.Resize((100,100)),\n",
        "                                                    transforms.ToTensor()]))\n",
        "\n",
        "im, target = cal_tech[2000]\n",
        "show_image(im)\n",
        "#print(im.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBs98pQKPOmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# letters={'A': 0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25,'space':26,'nothing':27}\n",
        "# for key in letters.keys():\n",
        "#   # define the name of the directory to be created\n",
        "#   path = \"/content/asl_alphabet_test/asl_alphabet_test/\"+key\n",
        "\n",
        "#   try:\n",
        "#       os.mkdir(path)\n",
        "#   except OSError:\n",
        "#       print (\"Creation of the directory %s failed\" % path)\n",
        "#   else:\n",
        "#       print (\"Successfully created the directory %s \" % path)\n",
        "# !cd asl_alphabet_test/asl_alphabet_test && ls\n",
        "# # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9kiB-HWPTiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# #shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/A_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/A/A_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/B_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/B/B_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/C_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/C/C_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/D_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/D/D_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/E_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/E/E_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/F_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/F/F_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/G_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/G/G_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/H_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/H/H_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/I_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/I/I_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/J_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/J/J_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/K_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/K/K_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/L_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/L/L_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/M_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/M/M_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/N_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/N/N_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/O_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/O/O_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/P_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/P/P_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/Q_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/Q/Q_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/R_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/R/R_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/S_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/S/S_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/T_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/T/T_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/U_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/U/U_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/V_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/V/V_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/W_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/W/W_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/X_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/X/X_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/Y_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/Y/Y_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/Z_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/Z/Z_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/nothing_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/nothing/nothing_test.jpg\")\n",
        "# shutil.move(\"/content/asl_alphabet_test/asl_alphabet_test/space_test.jpg\",\"/content/asl_alphabet_test/asl_alphabet_test/space/space_test.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}